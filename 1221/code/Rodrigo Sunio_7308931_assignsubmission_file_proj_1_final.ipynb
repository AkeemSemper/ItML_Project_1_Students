{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer:\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        filtered_tok = []\n",
    "        for tok in tokens :\n",
    "            if  tok not in self.stop_words :\n",
    "                filtered_tok.append(self.lemmatizer.lemmatize(tok))\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 - NLP and Text Classification\n",
    "\n",
    "For this project you will need to classify some angry comments into their respective category of angry. The process that you'll need to follow is (roughly):\n",
    "<ol>\n",
    "<li> Use NLP techniques to process the training data. \n",
    "<li> Train model(s) to predict which class(es) each comment is in.\n",
    "    <ul>\n",
    "    <li> A comment can belong to any number of classes, including none. \n",
    "    </ul>\n",
    "<li> Generate predictions for each of the comments in the test data. \n",
    "<li> Write your test data predicitions to a CSV file, which will be scored. \n",
    "</ol>\n",
    "\n",
    "You can use any models and NLP libraries you'd like. Think aobut the problem, look back to see if there's anything that might help, give it a try, and see if that helps. We've regularly said we have a \"toolkit\" of things that we can use, we generally don't know which ones we'll need, but here you have a pretty simple goal - if it makes it more accurate, it helps. There's not one specific solution here, there are lots of things that you could do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125096</th>\n",
       "      <td>9d21c2a21e0dc83a</td>\n",
       "      <td>Splitting the article\\nWhat do other editors t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106403</th>\n",
       "      <td>3944132a24173add</td>\n",
       "      <td>That sounds fine to me.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35052</th>\n",
       "      <td>5d9baed8989e9533</td>\n",
       "      <td>\"\\n\\n Happy holidays \\n\\n  Happy holidays. Bes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "125096  9d21c2a21e0dc83a  Splitting the article\\nWhat do other editors t...   \n",
       "106403  3944132a24173add                            That sounds fine to me.   \n",
       "35052   5d9baed8989e9533  \"\\n\\n Happy holidays \\n\\n  Happy holidays. Bes...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "125096      0             0        0       0       0              0  \n",
       "106403      0             0        0       0       0              0  \n",
       "35052       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\")\n",
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each column and create countplot\n",
    "for i, col in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    sns.countplot(x=col, data=train_df, ax=axs[i])\n",
    "    axs[i].set_xlabel(col, fontsize=12)\n",
    "    axs[i].set_ylabel('Count', fontsize=12)\n",
    "    axs[i].tick_params(labelsize=10)\n",
    "    axs[i].set_title('Distribution of ' + col, fontsize=12)\n",
    "\n",
    "# Adjust subplot spacing and display plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the above distribution of the target variables, the training dataset seems to be imbalanced. We may want to explore using oversampling methods such as SMOTE or ADASYN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following results were obtained from a RandomForestClassifier (RFC) model before and after SMOTE and ADASYN oversampling\n",
    "\"\"\"\n",
    "RFC Toxic_model before SMOTE\n",
    "AUC: 0.9326970920553436\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.99      0.97     36092\n",
    "           1       0.88      0.49      0.63      3801\n",
    "\n",
    "    accuracy                           0.94     39893\n",
    "   macro avg       0.92      0.74      0.80     39893\n",
    "weighted avg       0.94      0.94      0.94     39893\n",
    "\n",
    "RFC Toxic_model after SMOTE\n",
    "AUC: 0.9507946352291728\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.97      0.97     36082\n",
    "           1       0.70      0.68      0.69      3811\n",
    "\n",
    "    accuracy                           0.94     39893\n",
    "   macro avg       0.83      0.82      0.83     39893\n",
    "weighted avg       0.94      0.94      0.94     39893\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "RFC severe_toxic_model before SMOTE\n",
    "AUC: 0.9597362944570248\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      1.00     39501\n",
    "           1       0.59      0.13      0.22       392\n",
    "\n",
    "    accuracy                           0.99     39893\n",
    "   macro avg       0.79      0.57      0.61     39893\n",
    "weighted avg       0.99      0.99      0.99     39893\n",
    "\n",
    "After ADASYN\n",
    "AUC: 0.9397341602579613\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99     31594\n",
    "           1       0.32      0.38      0.35       321\n",
    "\n",
    "    accuracy                           0.99     31915\n",
    "   macro avg       0.66      0.69      0.67     31915\n",
    "weighted avg       0.99      0.99      0.99     31915\n",
    "\n",
    "After SMOTE\n",
    "AUC: 0.9384542433527245\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99     31594\n",
    "           1       0.34      0.38      0.36       321\n",
    "\n",
    "    accuracy                           0.99     31915\n",
    "   macro avg       0.67      0.69      0.68     31915\n",
    "weighted avg       0.99      0.99      0.99     31915\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally, the models' scores were improved after oversampling techniques particularly on f1-score and recall. However, it is also noticeable that the precision dropped on both models after oversampling. At the end, we tried the models on the test_df to predict the target variables but got surprising results.\n",
    "<ol>\n",
    "\n",
    "* The RFC models before oversampling techniques predicted 1378 toxic comments and zero(0) severe toxic comments.\n",
    "* After oversampling, the models predicted 429 toxic comments and 18129 severe toxic comments.\n",
    "* These results may indicate that the precision have been compromised after oversampling techniques.\n",
    "</ol>\n",
    "\n",
    "### We decided not to use oversampling technique and will just tune up the parameters inside the RFC model, i.e n_estimators, max_depth, class_weight etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring other suitable models and hyperparameters\n",
    "\n",
    "* We run the most common model for text classification, and to see each model's proformance and parameters for training model later.\n",
    "\n",
    "* MODELS : SVM, Decision Tree Classifier, Random Forest Classifier, Naive Bayes, logistic Regressior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from numpy import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_df = train_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = sample_train_df['comment_text']\n",
    "y = sample_train_df[\"obscene\"]\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params  = {\n",
    "    \"svm\" : {\n",
    "        \"model\":SVC(gamma=\"auto\"),\n",
    "        \"params\":{\n",
    "            'C' : [1,10,20],\n",
    "            'kernel':[\"rbf\"]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"decision_tree\":{\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\":{\n",
    "            'criterion':[\"entropy\",\"gini\"],\n",
    "            \"max_depth\":[5,8,9]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"random_forest\":{\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\":{\n",
    "            \"n_estimators\":[1,5,10],\n",
    "            \"max_depth\":[5,8,9]\n",
    "        }\n",
    "    },\n",
    "    \"naive_bayes\":{\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\":{}\n",
    "    },\n",
    "    \n",
    "    'logistic_regression' : {\n",
    "        'model' : LogisticRegression(solver='liblinear',multi_class = 'auto'),\n",
    "        'params': {\n",
    "            \"C\" : [1,5,10]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "for model_name,mp in model_params.items():\n",
    "    clf = GridSearchCV(mp[\"model\"],mp[\"params\"],cv=8,return_train_score=False)\n",
    "    clf.fit(x.toarray(), y)\n",
    "    score.append({\n",
    "        \"Model\" : model_name,\n",
    "        \"Best_Score\": clf.best_score_,\n",
    "        \"Best_Params\": clf.best_params_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best_Score</th>\n",
       "      <th>Best_Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.957</td>\n",
       "      <td>{'C': 1, 'kernel': 'rbf'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.971</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.958</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 5}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>0.952</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.963</td>\n",
       "      <td>{'C': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Best_Score                               Best_Params\n",
       "0                  svm       0.957                 {'C': 1, 'kernel': 'rbf'}\n",
       "1        decision_tree       0.971  {'criterion': 'entropy', 'max_depth': 8}\n",
       "2        random_forest       0.958       {'max_depth': 8, 'n_estimators': 5}\n",
       "3          naive_bayes       0.952                                        {}\n",
       "4  logistic_regression       0.963                                 {'C': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame(score,columns=[\"Model\",\"Best_Score\",\"Best_Params\"])\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 5000 samples\n",
    "Model\t                    Best_Score\tBest_Params\n",
    "0\tsvm\t                    0.946045\t{'C': 1, 'kernel': 'rbf'}\n",
    "1\tdecision_tree\t        0.972094\t{'criterion': 'entropy', 'max_depth': 5}\n",
    "2\trandom_forest\t        0.948029\t{'max_depth': 9, 'n_estimators': 5}\n",
    "3\tnaive_bayes\t            0.946045\t{}\n",
    "4\tlogistic_regression\t    0.964030\t{'C': 5}\n",
    "'''\n",
    "\n",
    "''' 8000 samples\n",
    "Model\t                    Best_Score\tBest_Params\n",
    "0\tsvm\t                    0.946500\t{'C': 20, 'kernel': 'rbf'}\n",
    "1\tdecision_tree\t        0.972500\t{'criterion': 'gini', 'max_depth': 8}\n",
    "2\trandom_forest\t        0.949125\t{'max_depth': 8, 'n_estimators': 1}\n",
    "3\tnaive_bayes\t            0.899625\t{}\n",
    "4\tlogistic_regression\t    0.972625\t{'C': 10}\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier, Random Forest Classifier, and logistic Regression seem to have better performance.\n",
    "\n",
    "But after a sample run, the Decision Tree Classifier and logistic Regression might have overfitting.\n",
    "\n",
    "We decided to go with Random Forest Classifier.\n",
    "\n",
    "To get the best parameters of the RFC model, we used GridsearchCV. We sampled out 30000 rows in order for the process to not take long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2_train_df = train_df.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = lemmaTokenizer(stop_words=stop_words)\n",
    "sample2_train_df['clean_text'] = sample2_train_df['comment_text'].apply(lambda x: tok(x))\n",
    "    \n",
    "model = Word2Vec(sample2_train_df['clean_text'], min_count=2, vector_size=300, sg=1)\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors)) \n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample2_train_df['clean_text'], sample2_train_df['toxic'])\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "    \n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'class_weight': 'balanced', 'max_features': 'sqrt', 'min_samples_leaf': 2}\n",
      "AUC score of the best model: 0.9379766299200556\n"
     ]
    }
   ],
   "source": [
    "# Using GridSearchCV to get the best parameters for RFC model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'min_samples_leaf': [1, 2],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'class_weight': ['balanced', None]}\n",
    "\n",
    "# Create a GridSearchCV object with 5-fold cross-validation\n",
    "rf_grid = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1),\n",
    "                       param_grid=param_grid, cv=5,\n",
    "                       scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rf_grid.fit(X_train_vectors_w2v, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding AUC score\n",
    "print(\"Best parameters found:\", rf_grid.best_params_)\n",
    "print(\"AUC score of the best model:\", rf_grid.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an RFC model to use for the whole training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "Use the training data to train your prediction model(s). Each of the classification output columns (toxic to the end) is a human label for the comment_text, assessing if it falls into that category of \"rude\". A comment may fall into any number of categories, or none at all. Membership in one output category is <b>independent</b> of membership in any of the other classes (think about this when you plan on how to make these predictions - it may also make it easier to split work amongst a team...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74148</th>\n",
       "      <td>c66284d99ab298c0</td>\n",
       "      <td>SCARANEESH SCARANEESH sell them childrenNoNonO...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145060</th>\n",
       "      <td>16bb8c622795ce6f</td>\n",
       "      <td>No, Really! \\n\\nYou're effing killing me!  You...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129425</th>\n",
       "      <td>b44e4b1c6bbaa881</td>\n",
       "      <td>Hi Pdfpdf, how are you? - Well, I really hate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "74148   c66284d99ab298c0  SCARANEESH SCARANEESH sell them childrenNoNonO...   \n",
       "145060  16bb8c622795ce6f  No, Really! \\n\\nYou're effing killing me!  You...   \n",
       "129425  b44e4b1c6bbaa881  Hi Pdfpdf, how are you? - Well, I really hate ...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "74148       0             0        0       0       0              0  \n",
       "145060      1             0        0       0       0              0  \n",
       "129425      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\")\n",
    "train_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, !, match, background, colour, 'm, seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, ,, 'm, really, trying, edit, war, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[,, sir, ,, hero, ., chance, remember, page, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, !, match, background, colour, 'm, seem...  \n",
       "2  [hey, man, ,, 'm, really, trying, edit, war, ....  \n",
       "3  [``, ca, n't, make, real, suggestion, improvem...  \n",
       "4  [,, sir, ,, hero, ., chance, remember, page, '...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "train_df[\"clean_text\"] = train_df[\"comment_text\"].apply(lambda x: tok(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>,</th>\n",
       "      <th>''</th>\n",
       "      <th>``</th>\n",
       "      <th>)</th>\n",
       "      <th>(</th>\n",
       "      <th>:</th>\n",
       "      <th>!</th>\n",
       "      <th>?</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>talkemail</th>\n",
       "      <th>histmerge</th>\n",
       "      <th>lfdder</th>\n",
       "      <th>staged</th>\n",
       "      <th>assimilating</th>\n",
       "      <th>uncyclopedic</th>\n",
       "      <th>ungrounded</th>\n",
       "      <th>anzick-1</th>\n",
       "      <th>kathmandu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058299</td>\n",
       "      <td>0.063848</td>\n",
       "      <td>-0.111571</td>\n",
       "      <td>-0.216432</td>\n",
       "      <td>0.164585</td>\n",
       "      <td>0.175037</td>\n",
       "      <td>0.392276</td>\n",
       "      <td>0.242116</td>\n",
       "      <td>-0.098903</td>\n",
       "      <td>0.225348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025431</td>\n",
       "      <td>0.011525</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>-0.005266</td>\n",
       "      <td>-0.014043</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>-0.002894</td>\n",
       "      <td>0.002230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.114778</td>\n",
       "      <td>0.236897</td>\n",
       "      <td>0.120887</td>\n",
       "      <td>0.335803</td>\n",
       "      <td>0.241774</td>\n",
       "      <td>0.352376</td>\n",
       "      <td>-0.067516</td>\n",
       "      <td>0.166901</td>\n",
       "      <td>0.231145</td>\n",
       "      <td>0.320674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097684</td>\n",
       "      <td>0.073848</td>\n",
       "      <td>0.103790</td>\n",
       "      <td>0.077264</td>\n",
       "      <td>0.137606</td>\n",
       "      <td>0.122488</td>\n",
       "      <td>0.128415</td>\n",
       "      <td>0.115227</td>\n",
       "      <td>0.117435</td>\n",
       "      <td>0.071649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.064374</td>\n",
       "      <td>-0.069493</td>\n",
       "      <td>-0.359475</td>\n",
       "      <td>-0.234695</td>\n",
       "      <td>-0.091920</td>\n",
       "      <td>-0.145977</td>\n",
       "      <td>-0.170341</td>\n",
       "      <td>0.288646</td>\n",
       "      <td>-0.062716</td>\n",
       "      <td>-0.130093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036696</td>\n",
       "      <td>0.028519</td>\n",
       "      <td>-0.004741</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>0.036094</td>\n",
       "      <td>0.026064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>0.142309</td>\n",
       "      <td>0.235862</td>\n",
       "      <td>0.248742</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.224071</td>\n",
       "      <td>0.503050</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>0.150408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030059</td>\n",
       "      <td>0.043572</td>\n",
       "      <td>0.023795</td>\n",
       "      <td>0.032215</td>\n",
       "      <td>0.036981</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>0.034538</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.032490</td>\n",
       "      <td>0.028736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.197908</td>\n",
       "      <td>-0.034272</td>\n",
       "      <td>-0.160430</td>\n",
       "      <td>-0.069454</td>\n",
       "      <td>0.128460</td>\n",
       "      <td>0.268523</td>\n",
       "      <td>0.097066</td>\n",
       "      <td>-0.202547</td>\n",
       "      <td>-0.318251</td>\n",
       "      <td>0.347130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062635</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.030052</td>\n",
       "      <td>0.057337</td>\n",
       "      <td>0.034262</td>\n",
       "      <td>0.041978</td>\n",
       "      <td>0.067468</td>\n",
       "      <td>0.051384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.266627</td>\n",
       "      <td>0.030192</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>0.115951</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.037061</td>\n",
       "      <td>-0.012226</td>\n",
       "      <td>0.201443</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.249479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111425</td>\n",
       "      <td>0.059554</td>\n",
       "      <td>0.085362</td>\n",
       "      <td>0.064425</td>\n",
       "      <td>0.109206</td>\n",
       "      <td>0.108968</td>\n",
       "      <td>0.134694</td>\n",
       "      <td>0.110749</td>\n",
       "      <td>0.090338</td>\n",
       "      <td>0.086387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.186444</td>\n",
       "      <td>-0.038521</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>0.077378</td>\n",
       "      <td>-0.016620</td>\n",
       "      <td>-0.284148</td>\n",
       "      <td>-0.282873</td>\n",
       "      <td>0.192456</td>\n",
       "      <td>0.356536</td>\n",
       "      <td>0.039197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>-0.012363</td>\n",
       "      <td>0.006566</td>\n",
       "      <td>-0.017081</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.004445</td>\n",
       "      <td>-0.022392</td>\n",
       "      <td>-0.003174</td>\n",
       "      <td>-0.008019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.069143</td>\n",
       "      <td>-0.160865</td>\n",
       "      <td>-0.076600</td>\n",
       "      <td>-0.095145</td>\n",
       "      <td>0.134724</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>-0.073932</td>\n",
       "      <td>-0.141942</td>\n",
       "      <td>-0.078528</td>\n",
       "      <td>0.058460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045868</td>\n",
       "      <td>-0.028019</td>\n",
       "      <td>-0.064818</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.053834</td>\n",
       "      <td>-0.047743</td>\n",
       "      <td>-0.054116</td>\n",
       "      <td>-0.057063</td>\n",
       "      <td>-0.065322</td>\n",
       "      <td>-0.048659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.293467</td>\n",
       "      <td>0.232963</td>\n",
       "      <td>-0.051979</td>\n",
       "      <td>0.140757</td>\n",
       "      <td>0.288311</td>\n",
       "      <td>0.251414</td>\n",
       "      <td>-0.036100</td>\n",
       "      <td>0.241023</td>\n",
       "      <td>0.359448</td>\n",
       "      <td>0.100984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071857</td>\n",
       "      <td>0.061324</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.077798</td>\n",
       "      <td>0.074978</td>\n",
       "      <td>0.060317</td>\n",
       "      <td>0.064575</td>\n",
       "      <td>0.070890</td>\n",
       "      <td>0.036693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.040028</td>\n",
       "      <td>-0.075811</td>\n",
       "      <td>0.010466</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>-0.008441</td>\n",
       "      <td>-0.044066</td>\n",
       "      <td>-0.254072</td>\n",
       "      <td>0.457695</td>\n",
       "      <td>-0.015724</td>\n",
       "      <td>-0.041249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.021408</td>\n",
       "      <td>-0.011611</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>-0.008514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  32574 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            .         ,        ''        ``         )         (         :  \\\n",
       "0    0.058299  0.063848 -0.111571 -0.216432  0.164585  0.175037  0.392276   \n",
       "1    0.114778  0.236897  0.120887  0.335803  0.241774  0.352376 -0.067516   \n",
       "2   -0.064374 -0.069493 -0.359475 -0.234695 -0.091920 -0.145977 -0.170341   \n",
       "3    0.244100  0.093584  0.142309  0.235862  0.248742  0.246011  0.224071   \n",
       "4   -0.197908 -0.034272 -0.160430 -0.069454  0.128460  0.268523  0.097066   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  0.266627  0.030192  0.169507  0.115951 -0.020497 -0.037061 -0.012226   \n",
       "296 -0.186444 -0.038521  0.016682  0.077378 -0.016620 -0.284148 -0.282873   \n",
       "297 -0.069143 -0.160865 -0.076600 -0.095145  0.134724  0.007841 -0.073932   \n",
       "298  0.293467  0.232963 -0.051979  0.140757  0.288311  0.251414 -0.036100   \n",
       "299 -0.040028 -0.075811  0.010466  0.033383 -0.008441 -0.044066 -0.254072   \n",
       "\n",
       "            !         ?   article  ...  multimedia  talkemail  histmerge  \\\n",
       "0    0.242116 -0.098903  0.225348  ...   -0.025431    0.011525   0.005302   \n",
       "1    0.166901  0.231145  0.320674  ...    0.097684    0.073848   0.103790   \n",
       "2    0.288646 -0.062716 -0.130093  ...    0.036696    0.028519  -0.004741   \n",
       "3    0.503050  0.064663  0.150408  ...    0.030059    0.043572   0.023795   \n",
       "4   -0.202547 -0.318251  0.347130  ...    0.062635    0.047556   0.034043   \n",
       "..        ...       ...       ...  ...         ...         ...        ...   \n",
       "295  0.201443  0.046361  0.249479  ...    0.111425    0.059554   0.085362   \n",
       "296  0.192456  0.356536  0.039197  ...   -0.006362    0.007466  -0.012363   \n",
       "297 -0.141942 -0.078528  0.058460  ...   -0.045868   -0.028019  -0.064818   \n",
       "298  0.241023  0.359448  0.100984  ...    0.071857    0.061324   0.067194   \n",
       "299  0.457695 -0.015724 -0.041249  ...    0.018542   -0.000460   0.005446   \n",
       "\n",
       "       lfdder    staged  assimilating  uncyclopedic  ungrounded  anzick-1  \\\n",
       "0   -0.005266 -0.014043      0.006204      0.006721    0.008879 -0.002894   \n",
       "1    0.077264  0.137606      0.122488      0.128415    0.115227  0.117435   \n",
       "2    0.031390  0.008031      0.036948      0.009968   -0.001592  0.036094   \n",
       "3    0.032215  0.036981      0.021840      0.034538    0.000646  0.032490   \n",
       "4    0.025600  0.030052      0.057337      0.034262    0.041978  0.067468   \n",
       "..        ...       ...           ...           ...         ...       ...   \n",
       "295  0.064425  0.109206      0.108968      0.134694    0.110749  0.090338   \n",
       "296  0.006566 -0.017081     -0.009305     -0.004445   -0.022392 -0.003174   \n",
       "297 -0.031648 -0.053834     -0.047743     -0.054116   -0.057063 -0.065322   \n",
       "298  0.050537  0.077798      0.074978      0.060317    0.064575  0.070890   \n",
       "299  0.016202  0.011715      0.016977      0.021408   -0.011611  0.009681   \n",
       "\n",
       "     kathmandu  \n",
       "0     0.002230  \n",
       "1     0.071649  \n",
       "2     0.026064  \n",
       "3     0.028736  \n",
       "4     0.051384  \n",
       "..         ...  \n",
       "295   0.086387  \n",
       "296  -0.008019  \n",
       "297  -0.048659  \n",
       "298   0.036693  \n",
       "299  -0.008514  \n",
       "\n",
       "[300 rows x 32574 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(train_df['clean_text'],min_count=2, vector_size=300, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 300) (39893, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055874</td>\n",
       "      <td>0.244217</td>\n",
       "      <td>-0.027840</td>\n",
       "      <td>-0.060415</td>\n",
       "      <td>-0.082235</td>\n",
       "      <td>-0.167085</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.305448</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.095022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119144</td>\n",
       "      <td>0.175716</td>\n",
       "      <td>0.141766</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>0.215210</td>\n",
       "      <td>0.257888</td>\n",
       "      <td>0.018178</td>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.152097</td>\n",
       "      <td>-0.009925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053953</td>\n",
       "      <td>0.215214</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>-0.042488</td>\n",
       "      <td>-0.109468</td>\n",
       "      <td>-0.239505</td>\n",
       "      <td>0.231558</td>\n",
       "      <td>0.223219</td>\n",
       "      <td>-0.125728</td>\n",
       "      <td>-0.171337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127274</td>\n",
       "      <td>0.220325</td>\n",
       "      <td>0.118515</td>\n",
       "      <td>0.070659</td>\n",
       "      <td>0.207721</td>\n",
       "      <td>0.247855</td>\n",
       "      <td>0.105462</td>\n",
       "      <td>-0.033805</td>\n",
       "      <td>0.104795</td>\n",
       "      <td>-0.178454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.108598</td>\n",
       "      <td>0.202354</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>-0.032542</td>\n",
       "      <td>-0.090465</td>\n",
       "      <td>-0.258095</td>\n",
       "      <td>0.260814</td>\n",
       "      <td>0.265755</td>\n",
       "      <td>-0.108393</td>\n",
       "      <td>-0.172577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136869</td>\n",
       "      <td>0.229317</td>\n",
       "      <td>0.139455</td>\n",
       "      <td>0.032105</td>\n",
       "      <td>0.234019</td>\n",
       "      <td>0.218607</td>\n",
       "      <td>-0.017373</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>-0.140709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.072870</td>\n",
       "      <td>0.186005</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>-0.029948</td>\n",
       "      <td>-0.095921</td>\n",
       "      <td>-0.236949</td>\n",
       "      <td>0.271339</td>\n",
       "      <td>0.254155</td>\n",
       "      <td>-0.171307</td>\n",
       "      <td>-0.149696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062223</td>\n",
       "      <td>0.146726</td>\n",
       "      <td>0.133057</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.221843</td>\n",
       "      <td>0.041877</td>\n",
       "      <td>-0.020056</td>\n",
       "      <td>0.165014</td>\n",
       "      <td>-0.065323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081205</td>\n",
       "      <td>0.171828</td>\n",
       "      <td>-0.023378</td>\n",
       "      <td>-0.086837</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>-0.167111</td>\n",
       "      <td>0.196479</td>\n",
       "      <td>0.208566</td>\n",
       "      <td>-0.070116</td>\n",
       "      <td>-0.064654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112008</td>\n",
       "      <td>0.201930</td>\n",
       "      <td>0.121813</td>\n",
       "      <td>0.066734</td>\n",
       "      <td>0.225279</td>\n",
       "      <td>0.264480</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.008985</td>\n",
       "      <td>0.074660</td>\n",
       "      <td>-0.127946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>0.054045</td>\n",
       "      <td>0.292326</td>\n",
       "      <td>-0.007830</td>\n",
       "      <td>-0.077363</td>\n",
       "      <td>-0.059526</td>\n",
       "      <td>-0.122240</td>\n",
       "      <td>0.346681</td>\n",
       "      <td>0.191058</td>\n",
       "      <td>-0.048837</td>\n",
       "      <td>-0.303597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163062</td>\n",
       "      <td>0.336950</td>\n",
       "      <td>0.143171</td>\n",
       "      <td>0.154567</td>\n",
       "      <td>0.211030</td>\n",
       "      <td>0.205145</td>\n",
       "      <td>0.045285</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.184903</td>\n",
       "      <td>-0.144073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.098819</td>\n",
       "      <td>0.202553</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>-0.061985</td>\n",
       "      <td>-0.182100</td>\n",
       "      <td>0.229319</td>\n",
       "      <td>0.229983</td>\n",
       "      <td>-0.106018</td>\n",
       "      <td>-0.171920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101122</td>\n",
       "      <td>0.239519</td>\n",
       "      <td>0.084001</td>\n",
       "      <td>0.012160</td>\n",
       "      <td>0.176329</td>\n",
       "      <td>0.238173</td>\n",
       "      <td>0.076461</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>0.138199</td>\n",
       "      <td>-0.132494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>-0.084719</td>\n",
       "      <td>0.223220</td>\n",
       "      <td>0.047946</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>-0.011268</td>\n",
       "      <td>-0.234971</td>\n",
       "      <td>0.203474</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>-0.200391</td>\n",
       "      <td>-0.073216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>0.113270</td>\n",
       "      <td>0.074173</td>\n",
       "      <td>-0.061381</td>\n",
       "      <td>0.188710</td>\n",
       "      <td>0.251434</td>\n",
       "      <td>0.023993</td>\n",
       "      <td>-0.099705</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>-0.037506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.064193</td>\n",
       "      <td>0.177578</td>\n",
       "      <td>0.018424</td>\n",
       "      <td>-0.072477</td>\n",
       "      <td>0.017225</td>\n",
       "      <td>-0.225646</td>\n",
       "      <td>0.218715</td>\n",
       "      <td>0.241853</td>\n",
       "      <td>-0.174249</td>\n",
       "      <td>-0.145652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075962</td>\n",
       "      <td>0.174471</td>\n",
       "      <td>0.050851</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>0.183389</td>\n",
       "      <td>0.242305</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>0.112419</td>\n",
       "      <td>-0.165793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.079344</td>\n",
       "      <td>0.161503</td>\n",
       "      <td>-0.005995</td>\n",
       "      <td>-0.170151</td>\n",
       "      <td>-0.026076</td>\n",
       "      <td>-0.172282</td>\n",
       "      <td>0.212747</td>\n",
       "      <td>0.331658</td>\n",
       "      <td>-0.164209</td>\n",
       "      <td>-0.100235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078210</td>\n",
       "      <td>0.128861</td>\n",
       "      <td>0.071555</td>\n",
       "      <td>0.057592</td>\n",
       "      <td>0.196676</td>\n",
       "      <td>0.185912</td>\n",
       "      <td>-0.074627</td>\n",
       "      <td>0.210064</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>-0.122399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.055874  0.244217 -0.027840 -0.060415 -0.082235 -0.167085  0.259382   \n",
       "1      -0.053953  0.215214 -0.011258 -0.042488 -0.109468 -0.239505  0.231558   \n",
       "2      -0.108598  0.202354 -0.001864 -0.032542 -0.090465 -0.258095  0.260814   \n",
       "3      -0.072870  0.186005  0.011692 -0.029948 -0.095921 -0.236949  0.271339   \n",
       "4      -0.081205  0.171828 -0.023378 -0.086837  0.004038 -0.167111  0.196479   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673  0.054045  0.292326 -0.007830 -0.077363 -0.059526 -0.122240  0.346681   \n",
       "119674 -0.098819  0.202553  0.018073 -0.017650 -0.061985 -0.182100  0.229319   \n",
       "119675 -0.084719  0.223220  0.047946  0.007822 -0.011268 -0.234971  0.203474   \n",
       "119676 -0.064193  0.177578  0.018424 -0.072477  0.017225 -0.225646  0.218715   \n",
       "119677 -0.079344  0.161503 -0.005995 -0.170151 -0.026076 -0.172282  0.212747   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.305448 -0.062403 -0.095022  ...  0.119144  0.175716  0.141766   \n",
       "1       0.223219 -0.125728 -0.171337  ...  0.127274  0.220325  0.118515   \n",
       "2       0.265755 -0.108393 -0.172577  ...  0.136869  0.229317  0.139455   \n",
       "3       0.254155 -0.171307 -0.149696  ...  0.062223  0.146726  0.133057   \n",
       "4       0.208566 -0.070116 -0.064654  ...  0.112008  0.201930  0.121813   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.191058 -0.048837 -0.303597  ...  0.163062  0.336950  0.143171   \n",
       "119674  0.229983 -0.106018 -0.171920  ...  0.101122  0.239519  0.084001   \n",
       "119675  0.145264 -0.200391 -0.073216  ...  0.004734  0.113270  0.074173   \n",
       "119676  0.241853 -0.174249 -0.145652  ...  0.075962  0.174471  0.050851   \n",
       "119677  0.331658 -0.164209 -0.100235  ...  0.078210  0.128861  0.071555   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.029149  0.215210  0.257888  0.018178  0.032511  0.152097 -0.009925  \n",
       "1       0.070659  0.207721  0.247855  0.105462 -0.033805  0.104795 -0.178454  \n",
       "2       0.032105  0.234019  0.218607 -0.017373  0.002259  0.173667 -0.140709  \n",
       "3      -0.004108  0.194000  0.221843  0.041877 -0.020056  0.165014 -0.065323  \n",
       "4       0.066734  0.225279  0.264480  0.024221  0.008985  0.074660 -0.127946  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673  0.154567  0.211030  0.205145  0.045285  0.005366  0.184903 -0.144073  \n",
       "119674  0.012160  0.176329  0.238173  0.076461 -0.039614  0.138199 -0.132494  \n",
       "119675 -0.061381  0.188710  0.251434  0.023993 -0.099705  0.267636 -0.037506  \n",
       "119676  0.009666  0.183389  0.242305  0.039473  0.059002  0.112419 -0.165793  \n",
       "119677  0.057592  0.196676  0.185912 -0.074627  0.210064  0.187487 -0.122399  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df[\"clean_text\"],train_df[\"toxic\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "print(X_train_vectors_w2v.shape, X_test_vectors_w2v.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.949851227559146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     36044\n",
      "           1       0.88      0.50      0.63      3849\n",
      "\n",
      "    accuracy                           0.94     39893\n",
      "   macro avg       0.91      0.74      0.80     39893\n",
      "weighted avg       0.94      0.94      0.94     39893\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_model = RandomForestClassifier(n_jobs = -1, random_state = 0, class_weight= 'balanced', max_features= 'sqrt', min_samples_leaf= 2)\n",
    "toxic_model.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = toxic_model.predict(X_test_vectors_w2v)\n",
    "y_prob = toxic_model.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Severe Toxic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 300) (39893, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.108042</td>\n",
       "      <td>0.156638</td>\n",
       "      <td>-0.041522</td>\n",
       "      <td>-0.035062</td>\n",
       "      <td>-0.135402</td>\n",
       "      <td>-0.137155</td>\n",
       "      <td>0.204557</td>\n",
       "      <td>0.212602</td>\n",
       "      <td>-0.147791</td>\n",
       "      <td>-0.152470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118360</td>\n",
       "      <td>0.206885</td>\n",
       "      <td>0.046809</td>\n",
       "      <td>0.067368</td>\n",
       "      <td>0.229434</td>\n",
       "      <td>0.254362</td>\n",
       "      <td>0.030373</td>\n",
       "      <td>-0.080299</td>\n",
       "      <td>0.180218</td>\n",
       "      <td>-0.100414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.073021</td>\n",
       "      <td>0.279651</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>-0.005365</td>\n",
       "      <td>-0.131429</td>\n",
       "      <td>-0.234896</td>\n",
       "      <td>0.210465</td>\n",
       "      <td>0.206126</td>\n",
       "      <td>-0.049733</td>\n",
       "      <td>-0.179938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075678</td>\n",
       "      <td>0.164678</td>\n",
       "      <td>0.020385</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.172396</td>\n",
       "      <td>0.193977</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>-0.080601</td>\n",
       "      <td>0.169967</td>\n",
       "      <td>-0.150574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.066630</td>\n",
       "      <td>0.216254</td>\n",
       "      <td>0.079079</td>\n",
       "      <td>-0.196406</td>\n",
       "      <td>-0.245198</td>\n",
       "      <td>-0.210012</td>\n",
       "      <td>0.111040</td>\n",
       "      <td>0.363143</td>\n",
       "      <td>-0.155034</td>\n",
       "      <td>-0.194449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.176475</td>\n",
       "      <td>0.123415</td>\n",
       "      <td>0.103965</td>\n",
       "      <td>0.310288</td>\n",
       "      <td>-0.037128</td>\n",
       "      <td>0.115647</td>\n",
       "      <td>0.142655</td>\n",
       "      <td>-0.052012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.125403</td>\n",
       "      <td>0.207016</td>\n",
       "      <td>-0.061289</td>\n",
       "      <td>-0.070930</td>\n",
       "      <td>0.056797</td>\n",
       "      <td>-0.232463</td>\n",
       "      <td>0.177478</td>\n",
       "      <td>0.317782</td>\n",
       "      <td>-0.117417</td>\n",
       "      <td>-0.002012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114313</td>\n",
       "      <td>0.163466</td>\n",
       "      <td>0.115086</td>\n",
       "      <td>0.050256</td>\n",
       "      <td>0.119965</td>\n",
       "      <td>0.300316</td>\n",
       "      <td>0.017684</td>\n",
       "      <td>0.069137</td>\n",
       "      <td>0.167682</td>\n",
       "      <td>-0.109673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.111935</td>\n",
       "      <td>0.163633</td>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>-0.053025</td>\n",
       "      <td>-0.188579</td>\n",
       "      <td>0.229431</td>\n",
       "      <td>0.211488</td>\n",
       "      <td>-0.110937</td>\n",
       "      <td>-0.068533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099264</td>\n",
       "      <td>0.161415</td>\n",
       "      <td>0.081064</td>\n",
       "      <td>0.066340</td>\n",
       "      <td>0.193965</td>\n",
       "      <td>0.282537</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.043141</td>\n",
       "      <td>0.173621</td>\n",
       "      <td>-0.126924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>-0.068043</td>\n",
       "      <td>0.203892</td>\n",
       "      <td>0.027149</td>\n",
       "      <td>-0.024197</td>\n",
       "      <td>-0.060990</td>\n",
       "      <td>-0.199811</td>\n",
       "      <td>0.251073</td>\n",
       "      <td>0.206016</td>\n",
       "      <td>-0.088707</td>\n",
       "      <td>-0.098704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.189139</td>\n",
       "      <td>0.012299</td>\n",
       "      <td>0.025064</td>\n",
       "      <td>0.210189</td>\n",
       "      <td>0.253308</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>-0.016106</td>\n",
       "      <td>0.164757</td>\n",
       "      <td>-0.141830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.122918</td>\n",
       "      <td>0.185832</td>\n",
       "      <td>0.015470</td>\n",
       "      <td>-0.036387</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>-0.254229</td>\n",
       "      <td>0.216537</td>\n",
       "      <td>0.257691</td>\n",
       "      <td>-0.264525</td>\n",
       "      <td>-0.169508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158197</td>\n",
       "      <td>0.241428</td>\n",
       "      <td>0.071241</td>\n",
       "      <td>-0.062869</td>\n",
       "      <td>0.185559</td>\n",
       "      <td>0.203096</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>0.055172</td>\n",
       "      <td>0.239271</td>\n",
       "      <td>-0.056701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>-0.143477</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>-0.039520</td>\n",
       "      <td>-0.029032</td>\n",
       "      <td>-0.022138</td>\n",
       "      <td>-0.162562</td>\n",
       "      <td>0.129778</td>\n",
       "      <td>0.246858</td>\n",
       "      <td>-0.013143</td>\n",
       "      <td>-0.038688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.150783</td>\n",
       "      <td>0.152634</td>\n",
       "      <td>0.135537</td>\n",
       "      <td>0.259567</td>\n",
       "      <td>0.287416</td>\n",
       "      <td>-0.146267</td>\n",
       "      <td>0.061453</td>\n",
       "      <td>0.153832</td>\n",
       "      <td>-0.098118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.078839</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>-0.036319</td>\n",
       "      <td>-0.061478</td>\n",
       "      <td>-0.187047</td>\n",
       "      <td>0.187420</td>\n",
       "      <td>0.254713</td>\n",
       "      <td>-0.110562</td>\n",
       "      <td>-0.095045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067915</td>\n",
       "      <td>0.210739</td>\n",
       "      <td>0.127247</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.165239</td>\n",
       "      <td>0.261055</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>0.152481</td>\n",
       "      <td>-0.104082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.101621</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>0.007907</td>\n",
       "      <td>-0.077585</td>\n",
       "      <td>-0.172890</td>\n",
       "      <td>0.179220</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>-0.033846</td>\n",
       "      <td>-0.128648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133523</td>\n",
       "      <td>0.163066</td>\n",
       "      <td>0.143302</td>\n",
       "      <td>0.010888</td>\n",
       "      <td>0.240094</td>\n",
       "      <td>0.239376</td>\n",
       "      <td>-0.011555</td>\n",
       "      <td>-0.063795</td>\n",
       "      <td>0.147199</td>\n",
       "      <td>-0.107958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.108042  0.156638 -0.041522 -0.035062 -0.135402 -0.137155  0.204557   \n",
       "1      -0.073021  0.279651 -0.026031 -0.005365 -0.131429 -0.234896  0.210465   \n",
       "2      -0.066630  0.216254  0.079079 -0.196406 -0.245198 -0.210012  0.111040   \n",
       "3      -0.125403  0.207016 -0.061289 -0.070930  0.056797 -0.232463  0.177478   \n",
       "4      -0.111935  0.163633  0.012288  0.018377 -0.053025 -0.188579  0.229431   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673 -0.068043  0.203892  0.027149 -0.024197 -0.060990 -0.199811  0.251073   \n",
       "119674 -0.122918  0.185832  0.015470 -0.036387 -0.010865 -0.254229  0.216537   \n",
       "119675 -0.143477  0.165400 -0.039520 -0.029032 -0.022138 -0.162562  0.129778   \n",
       "119676 -0.078839  0.091856  0.019641 -0.036319 -0.061478 -0.187047  0.187420   \n",
       "119677 -0.101621  0.180469 -0.027732  0.007907 -0.077585 -0.172890  0.179220   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.212602 -0.147791 -0.152470  ...  0.118360  0.206885  0.046809   \n",
       "1       0.206126 -0.049733 -0.179938  ...  0.075678  0.164678  0.020385   \n",
       "2       0.363143 -0.155034 -0.194449  ...  0.124836  0.053204  0.176475   \n",
       "3       0.317782 -0.117417 -0.002012  ...  0.114313  0.163466  0.115086   \n",
       "4       0.211488 -0.110937 -0.068533  ...  0.099264  0.161415  0.081064   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.206016 -0.088707 -0.098704  ...  0.114613  0.189139  0.012299   \n",
       "119674  0.257691 -0.264525 -0.169508  ...  0.158197  0.241428  0.071241   \n",
       "119675  0.246858 -0.013143 -0.038688  ...  0.043011  0.150783  0.152634   \n",
       "119676  0.254713 -0.110562 -0.095045  ...  0.067915  0.210739  0.127247   \n",
       "119677  0.165780 -0.033846 -0.128648  ...  0.133523  0.163066  0.143302   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.067368  0.229434  0.254362  0.030373 -0.080299  0.180218 -0.100414  \n",
       "1       0.000299  0.172396  0.193977  0.111094 -0.080601  0.169967 -0.150574  \n",
       "2       0.123415  0.103965  0.310288 -0.037128  0.115647  0.142655 -0.052012  \n",
       "3       0.050256  0.119965  0.300316  0.017684  0.069137  0.167682 -0.109673  \n",
       "4       0.066340  0.193965  0.282537  0.007635  0.043141  0.173621 -0.126924  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673  0.025064  0.210189  0.253308 -0.000198 -0.016106  0.164757 -0.141830  \n",
       "119674 -0.062869  0.185559  0.203096  0.005351  0.055172  0.239271 -0.056701  \n",
       "119675  0.135537  0.259567  0.287416 -0.146267  0.061453  0.153832 -0.098118  \n",
       "119676  0.045300  0.165239  0.261055  0.000504  0.041157  0.152481 -0.104082  \n",
       "119677  0.010888  0.240094  0.239376 -0.011555 -0.063795  0.147199 -0.107958  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train_ST, X_test_ST, y_train_ST, y_test_ST = train_test_split(train_df[\"clean_text\"],train_df[\"severe_toxic\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw_ST = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v_ST = modelw_ST.transform(X_train_ST)\n",
    "X_test_vectors_w2v_ST = modelw_ST.transform(X_test_ST)\n",
    "print(X_train_vectors_w2v_ST.shape, X_test_vectors_w2v_ST.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v_ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.959362699624393\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     39522\n",
      "           1       0.47      0.20      0.28       371\n",
      "\n",
      "    accuracy                           0.99     39893\n",
      "   macro avg       0.73      0.60      0.64     39893\n",
      "weighted avg       0.99      0.99      0.99     39893\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "severetoxic_model = RandomForestClassifier(n_jobs = -1, random_state = 0, class_weight= 'balanced', max_features= 'sqrt', min_samples_leaf= 2)\n",
    "severetoxic_model.fit(X_train_vectors_w2v_ST, y_train_ST)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "ST_y_predict = severetoxic_model.predict(X_test_vectors_w2v_ST)\n",
    "ST_y_prob = severetoxic_model.predict_proba(X_test_vectors_w2v_ST)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_ST, ST_y_prob)\n",
    "ST_roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', ST_roc_auc)  \n",
    "\n",
    "print(classification_report(y_test_ST, ST_y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test_ST, ST_y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.150814</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>-0.008540</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-0.035493</td>\n",
       "      <td>-0.174374</td>\n",
       "      <td>0.136232</td>\n",
       "      <td>0.150867</td>\n",
       "      <td>-0.021170</td>\n",
       "      <td>-0.059598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039005</td>\n",
       "      <td>0.124294</td>\n",
       "      <td>0.069568</td>\n",
       "      <td>0.078942</td>\n",
       "      <td>0.227579</td>\n",
       "      <td>0.299737</td>\n",
       "      <td>0.022350</td>\n",
       "      <td>-0.041665</td>\n",
       "      <td>0.148746</td>\n",
       "      <td>-0.069853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.088561</td>\n",
       "      <td>0.143863</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.095056</td>\n",
       "      <td>-0.168741</td>\n",
       "      <td>0.183983</td>\n",
       "      <td>0.198263</td>\n",
       "      <td>-0.095520</td>\n",
       "      <td>-0.068456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053793</td>\n",
       "      <td>0.222241</td>\n",
       "      <td>0.051117</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>0.212601</td>\n",
       "      <td>0.283884</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>-0.015158</td>\n",
       "      <td>0.152336</td>\n",
       "      <td>-0.124620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.193703</td>\n",
       "      <td>0.208211</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>-0.067735</td>\n",
       "      <td>-0.068631</td>\n",
       "      <td>-0.252522</td>\n",
       "      <td>0.318760</td>\n",
       "      <td>0.274150</td>\n",
       "      <td>-0.181196</td>\n",
       "      <td>-0.174775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161611</td>\n",
       "      <td>0.284070</td>\n",
       "      <td>0.031099</td>\n",
       "      <td>-0.057004</td>\n",
       "      <td>0.247646</td>\n",
       "      <td>0.309179</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>-0.050567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020725</td>\n",
       "      <td>0.185664</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>-0.239701</td>\n",
       "      <td>0.159911</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>-0.148415</td>\n",
       "      <td>-0.116835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074697</td>\n",
       "      <td>0.217242</td>\n",
       "      <td>0.126363</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.152890</td>\n",
       "      <td>0.294206</td>\n",
       "      <td>0.065936</td>\n",
       "      <td>-0.064537</td>\n",
       "      <td>0.211246</td>\n",
       "      <td>-0.089004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.027852</td>\n",
       "      <td>0.182297</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>-0.051360</td>\n",
       "      <td>-0.094253</td>\n",
       "      <td>-0.175907</td>\n",
       "      <td>0.282694</td>\n",
       "      <td>0.211447</td>\n",
       "      <td>-0.191843</td>\n",
       "      <td>-0.074109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080605</td>\n",
       "      <td>0.164350</td>\n",
       "      <td>0.117475</td>\n",
       "      <td>0.021897</td>\n",
       "      <td>0.169681</td>\n",
       "      <td>0.228924</td>\n",
       "      <td>-0.020053</td>\n",
       "      <td>0.031920</td>\n",
       "      <td>0.257276</td>\n",
       "      <td>-0.102856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>-0.063905</td>\n",
       "      <td>0.201146</td>\n",
       "      <td>-0.049369</td>\n",
       "      <td>-0.129445</td>\n",
       "      <td>0.051582</td>\n",
       "      <td>-0.118512</td>\n",
       "      <td>0.129631</td>\n",
       "      <td>0.234729</td>\n",
       "      <td>-0.064194</td>\n",
       "      <td>-0.074314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065878</td>\n",
       "      <td>0.208771</td>\n",
       "      <td>0.235370</td>\n",
       "      <td>0.072248</td>\n",
       "      <td>0.167264</td>\n",
       "      <td>0.202817</td>\n",
       "      <td>-0.059871</td>\n",
       "      <td>0.057374</td>\n",
       "      <td>0.113993</td>\n",
       "      <td>-0.056375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.018989</td>\n",
       "      <td>0.149368</td>\n",
       "      <td>0.015391</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>-0.114702</td>\n",
       "      <td>-0.117457</td>\n",
       "      <td>0.216463</td>\n",
       "      <td>0.193501</td>\n",
       "      <td>-0.137394</td>\n",
       "      <td>-0.131293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053133</td>\n",
       "      <td>0.216039</td>\n",
       "      <td>0.108002</td>\n",
       "      <td>0.061632</td>\n",
       "      <td>0.253770</td>\n",
       "      <td>0.250723</td>\n",
       "      <td>0.022327</td>\n",
       "      <td>-0.072368</td>\n",
       "      <td>0.091177</td>\n",
       "      <td>-0.119966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>-0.127909</td>\n",
       "      <td>0.168548</td>\n",
       "      <td>0.050497</td>\n",
       "      <td>-0.068904</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>-0.194427</td>\n",
       "      <td>0.205872</td>\n",
       "      <td>0.264476</td>\n",
       "      <td>-0.117791</td>\n",
       "      <td>-0.145603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108791</td>\n",
       "      <td>0.217551</td>\n",
       "      <td>0.197318</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>0.248100</td>\n",
       "      <td>0.193459</td>\n",
       "      <td>0.070677</td>\n",
       "      <td>0.144071</td>\n",
       "      <td>0.153628</td>\n",
       "      <td>-0.079993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.136667</td>\n",
       "      <td>0.194543</td>\n",
       "      <td>0.024781</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>-0.090053</td>\n",
       "      <td>-0.078584</td>\n",
       "      <td>0.230408</td>\n",
       "      <td>0.210659</td>\n",
       "      <td>-0.100205</td>\n",
       "      <td>-0.101932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059923</td>\n",
       "      <td>0.190905</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>-0.007409</td>\n",
       "      <td>0.188631</td>\n",
       "      <td>0.257371</td>\n",
       "      <td>0.054030</td>\n",
       "      <td>-0.044089</td>\n",
       "      <td>0.162114</td>\n",
       "      <td>-0.155728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.012133</td>\n",
       "      <td>0.144851</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>-0.037978</td>\n",
       "      <td>-0.239541</td>\n",
       "      <td>0.240625</td>\n",
       "      <td>0.287233</td>\n",
       "      <td>-0.142841</td>\n",
       "      <td>-0.242794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040526</td>\n",
       "      <td>0.237612</td>\n",
       "      <td>0.294829</td>\n",
       "      <td>0.104850</td>\n",
       "      <td>0.181504</td>\n",
       "      <td>0.271235</td>\n",
       "      <td>-0.037411</td>\n",
       "      <td>-0.045262</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>-0.145407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.150814  0.153558 -0.008540 -0.001628 -0.035493 -0.174374  0.136232   \n",
       "1      -0.088561  0.143863 -0.014652 -0.000072 -0.095056 -0.168741  0.183983   \n",
       "2      -0.193703  0.208211  0.078471 -0.067735 -0.068631 -0.252522  0.318760   \n",
       "3      -0.020725  0.185664  0.020734  0.043300  0.042138 -0.239701  0.159911   \n",
       "4      -0.027852  0.182297  0.003964 -0.051360 -0.094253 -0.175907  0.282694   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673 -0.063905  0.201146 -0.049369 -0.129445  0.051582 -0.118512  0.129631   \n",
       "119674 -0.018989  0.149368  0.015391  0.012818 -0.114702 -0.117457  0.216463   \n",
       "119675 -0.127909  0.168548  0.050497 -0.068904  0.047797 -0.194427  0.205872   \n",
       "119676 -0.136667  0.194543  0.024781  0.009255 -0.090053 -0.078584  0.230408   \n",
       "119677 -0.012133  0.144851  0.085471 -0.001845 -0.037978 -0.239541  0.240625   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.150867 -0.021170 -0.059598  ...  0.039005  0.124294  0.069568   \n",
       "1       0.198263 -0.095520 -0.068456  ...  0.053793  0.222241  0.051117   \n",
       "2       0.274150 -0.181196 -0.174775  ...  0.161611  0.284070  0.031099   \n",
       "3       0.077159 -0.148415 -0.116835  ...  0.074697  0.217242  0.126363   \n",
       "4       0.211447 -0.191843 -0.074109  ...  0.080605  0.164350  0.117475   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.234729 -0.064194 -0.074314  ...  0.065878  0.208771  0.235370   \n",
       "119674  0.193501 -0.137394 -0.131293  ...  0.053133  0.216039  0.108002   \n",
       "119675  0.264476 -0.117791 -0.145603  ...  0.108791  0.217551  0.197318   \n",
       "119676  0.210659 -0.100205 -0.101932  ...  0.059923  0.190905  0.066596   \n",
       "119677  0.287233 -0.142841 -0.242794  ...  0.040526  0.237612  0.294829   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.078942  0.227579  0.299737  0.022350 -0.041665  0.148746 -0.069853  \n",
       "1       0.032268  0.212601  0.283884  0.051682 -0.015158  0.152336 -0.124620  \n",
       "2      -0.057004  0.247646  0.309179  0.003328  0.129336  0.193605 -0.050567  \n",
       "3       0.063694  0.152890  0.294206  0.065936 -0.064537  0.211246 -0.089004  \n",
       "4       0.021897  0.169681  0.228924 -0.020053  0.031920  0.257276 -0.102856  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673  0.072248  0.167264  0.202817 -0.059871  0.057374  0.113993 -0.056375  \n",
       "119674  0.061632  0.253770  0.250723  0.022327 -0.072368  0.091177 -0.119966  \n",
       "119675  0.029490  0.248100  0.193459  0.070677  0.144071  0.153628 -0.079993  \n",
       "119676 -0.007409  0.188631  0.257371  0.054030 -0.044089  0.162114 -0.155728  \n",
       "119677  0.104850  0.181504  0.271235 -0.037411 -0.045262  0.167127 -0.145407  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train_Th, X_test_Th, y_train_Th, y_test_Th = train_test_split(train_df[\"clean_text\"],train_df[\"threat\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw_Th = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v_Th = modelw_Th.transform(X_train_Th)\n",
    "X_test_vectors_w2v_Th = modelw_Th.transform(X_test_Th)\n",
    "pd.DataFrame(X_train_vectors_w2v_Th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9225560182431094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39770\n",
      "           1       0.67      0.08      0.14       123\n",
      "\n",
      "    accuracy                           1.00     39893\n",
      "   macro avg       0.83      0.54      0.57     39893\n",
      "weighted avg       1.00      1.00      1.00     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "threat_model = RandomForestClassifier(n_jobs = -1, random_state = 0, class_weight= 'balanced', max_features= 'sqrt', min_samples_leaf= 2)\n",
    "threat_model.fit(X_train_vectors_w2v_Th, y_train_Th)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "Th_y_predict = threat_model.predict(X_test_vectors_w2v_Th)\n",
    "Th_y_prob = threat_model.predict_proba(X_test_vectors_w2v_Th)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_Th, Th_y_prob)\n",
    "Th_roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', Th_roc_auc)  \n",
    "\n",
    "print(classification_report(y_test_Th, Th_y_predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity Hate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 300) (39893, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.052452</td>\n",
       "      <td>0.247858</td>\n",
       "      <td>-0.068585</td>\n",
       "      <td>0.058882</td>\n",
       "      <td>-0.113417</td>\n",
       "      <td>-0.180910</td>\n",
       "      <td>0.333305</td>\n",
       "      <td>0.185624</td>\n",
       "      <td>-0.103139</td>\n",
       "      <td>-0.079784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111840</td>\n",
       "      <td>0.129977</td>\n",
       "      <td>-0.002319</td>\n",
       "      <td>-0.031646</td>\n",
       "      <td>0.284053</td>\n",
       "      <td>0.329334</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.196423</td>\n",
       "      <td>-0.168426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.070346</td>\n",
       "      <td>0.268159</td>\n",
       "      <td>-0.020783</td>\n",
       "      <td>-0.022888</td>\n",
       "      <td>-0.134959</td>\n",
       "      <td>-0.144370</td>\n",
       "      <td>0.161459</td>\n",
       "      <td>0.184464</td>\n",
       "      <td>-0.154817</td>\n",
       "      <td>-0.139936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055656</td>\n",
       "      <td>0.099790</td>\n",
       "      <td>0.213826</td>\n",
       "      <td>-0.052687</td>\n",
       "      <td>0.305880</td>\n",
       "      <td>0.148807</td>\n",
       "      <td>0.036678</td>\n",
       "      <td>0.064346</td>\n",
       "      <td>0.119069</td>\n",
       "      <td>-0.174116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138034</td>\n",
       "      <td>0.197527</td>\n",
       "      <td>-0.038410</td>\n",
       "      <td>-0.072825</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>-0.202648</td>\n",
       "      <td>0.195165</td>\n",
       "      <td>0.155370</td>\n",
       "      <td>-0.142116</td>\n",
       "      <td>-0.134152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.189657</td>\n",
       "      <td>0.142415</td>\n",
       "      <td>0.038336</td>\n",
       "      <td>0.245078</td>\n",
       "      <td>0.225058</td>\n",
       "      <td>0.094458</td>\n",
       "      <td>-0.074817</td>\n",
       "      <td>0.196084</td>\n",
       "      <td>-0.082121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.120050</td>\n",
       "      <td>0.237035</td>\n",
       "      <td>0.042883</td>\n",
       "      <td>-0.038957</td>\n",
       "      <td>-0.072957</td>\n",
       "      <td>-0.234723</td>\n",
       "      <td>0.245809</td>\n",
       "      <td>0.241853</td>\n",
       "      <td>-0.188203</td>\n",
       "      <td>-0.181862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075931</td>\n",
       "      <td>0.167329</td>\n",
       "      <td>0.055622</td>\n",
       "      <td>-0.022901</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.223812</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>-0.053800</td>\n",
       "      <td>0.222471</td>\n",
       "      <td>-0.118742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.065523</td>\n",
       "      <td>0.174026</td>\n",
       "      <td>0.070441</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>-0.073562</td>\n",
       "      <td>-0.062102</td>\n",
       "      <td>0.237466</td>\n",
       "      <td>0.178197</td>\n",
       "      <td>-0.061490</td>\n",
       "      <td>-0.030173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034646</td>\n",
       "      <td>0.193166</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.064305</td>\n",
       "      <td>0.232668</td>\n",
       "      <td>0.248519</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>-0.032123</td>\n",
       "      <td>0.166828</td>\n",
       "      <td>-0.069573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>-0.130976</td>\n",
       "      <td>0.132688</td>\n",
       "      <td>-0.018329</td>\n",
       "      <td>-0.157532</td>\n",
       "      <td>-0.082035</td>\n",
       "      <td>-0.259800</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.281923</td>\n",
       "      <td>-0.119407</td>\n",
       "      <td>0.010344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091920</td>\n",
       "      <td>0.223034</td>\n",
       "      <td>0.128904</td>\n",
       "      <td>0.084714</td>\n",
       "      <td>0.211275</td>\n",
       "      <td>0.227555</td>\n",
       "      <td>-0.118160</td>\n",
       "      <td>0.228542</td>\n",
       "      <td>0.205625</td>\n",
       "      <td>-0.052172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.112543</td>\n",
       "      <td>0.204797</td>\n",
       "      <td>0.036526</td>\n",
       "      <td>-0.077202</td>\n",
       "      <td>-0.020043</td>\n",
       "      <td>-0.211659</td>\n",
       "      <td>0.161597</td>\n",
       "      <td>0.255466</td>\n",
       "      <td>-0.062101</td>\n",
       "      <td>-0.179006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126122</td>\n",
       "      <td>0.209783</td>\n",
       "      <td>0.128253</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.181886</td>\n",
       "      <td>0.226069</td>\n",
       "      <td>0.073899</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.171068</td>\n",
       "      <td>-0.157920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.184602</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>-0.130647</td>\n",
       "      <td>-0.046782</td>\n",
       "      <td>-0.223671</td>\n",
       "      <td>0.120573</td>\n",
       "      <td>0.255806</td>\n",
       "      <td>-0.127804</td>\n",
       "      <td>-0.140019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209366</td>\n",
       "      <td>0.233613</td>\n",
       "      <td>0.170595</td>\n",
       "      <td>0.110919</td>\n",
       "      <td>0.291915</td>\n",
       "      <td>0.228631</td>\n",
       "      <td>-0.018176</td>\n",
       "      <td>-0.035796</td>\n",
       "      <td>0.095518</td>\n",
       "      <td>-0.125593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.063514</td>\n",
       "      <td>0.236476</td>\n",
       "      <td>-0.026296</td>\n",
       "      <td>-0.061830</td>\n",
       "      <td>-0.007000</td>\n",
       "      <td>-0.280811</td>\n",
       "      <td>0.146132</td>\n",
       "      <td>0.350215</td>\n",
       "      <td>-0.187345</td>\n",
       "      <td>-0.151974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182518</td>\n",
       "      <td>0.184688</td>\n",
       "      <td>0.136868</td>\n",
       "      <td>-0.009973</td>\n",
       "      <td>0.201993</td>\n",
       "      <td>0.227549</td>\n",
       "      <td>-0.009073</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>0.168123</td>\n",
       "      <td>-0.176607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.125131</td>\n",
       "      <td>0.182710</td>\n",
       "      <td>0.135562</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>-0.091010</td>\n",
       "      <td>-0.206962</td>\n",
       "      <td>0.160380</td>\n",
       "      <td>0.293393</td>\n",
       "      <td>-0.107472</td>\n",
       "      <td>-0.095862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086829</td>\n",
       "      <td>0.203758</td>\n",
       "      <td>0.111635</td>\n",
       "      <td>0.051147</td>\n",
       "      <td>0.148358</td>\n",
       "      <td>0.266412</td>\n",
       "      <td>0.074912</td>\n",
       "      <td>0.055924</td>\n",
       "      <td>0.182045</td>\n",
       "      <td>-0.136412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.052452  0.247858 -0.068585  0.058882 -0.113417 -0.180910  0.333305   \n",
       "1      -0.070346  0.268159 -0.020783 -0.022888 -0.134959 -0.144370  0.161459   \n",
       "2      -0.138034  0.197527 -0.038410 -0.072825  0.035153 -0.202648  0.195165   \n",
       "3      -0.120050  0.237035  0.042883 -0.038957 -0.072957 -0.234723  0.245809   \n",
       "4      -0.065523  0.174026  0.070441  0.014119 -0.073562 -0.062102  0.237466   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673 -0.130976  0.132688 -0.018329 -0.157532 -0.082035 -0.259800  0.241800   \n",
       "119674 -0.112543  0.204797  0.036526 -0.077202 -0.020043 -0.211659  0.161597   \n",
       "119675  0.017625  0.184602  0.006417 -0.130647 -0.046782 -0.223671  0.120573   \n",
       "119676 -0.063514  0.236476 -0.026296 -0.061830 -0.007000 -0.280811  0.146132   \n",
       "119677 -0.125131  0.182710  0.135562  0.027115 -0.091010 -0.206962  0.160380   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.185624 -0.103139 -0.079784  ...  0.111840  0.129977 -0.002319   \n",
       "1       0.184464 -0.154817 -0.139936  ...  0.055656  0.099790  0.213826   \n",
       "2       0.155370 -0.142116 -0.134152  ...  0.052224  0.189657  0.142415   \n",
       "3       0.241853 -0.188203 -0.181862  ...  0.075931  0.167329  0.055622   \n",
       "4       0.178197 -0.061490 -0.030173  ...  0.034646  0.193166  0.078200   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.281923 -0.119407  0.010344  ...  0.091920  0.223034  0.128904   \n",
       "119674  0.255466 -0.062101 -0.179006  ...  0.126122  0.209783  0.128253   \n",
       "119675  0.255806 -0.127804 -0.140019  ...  0.209366  0.233613  0.170595   \n",
       "119676  0.350215 -0.187345 -0.151974  ...  0.182518  0.184688  0.136868   \n",
       "119677  0.293393 -0.107472 -0.095862  ...  0.086829  0.203758  0.111635   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0      -0.031646  0.284053  0.329334 -0.010977  0.094444  0.196423 -0.168426  \n",
       "1      -0.052687  0.305880  0.148807  0.036678  0.064346  0.119069 -0.174116  \n",
       "2       0.038336  0.245078  0.225058  0.094458 -0.074817  0.196084 -0.082121  \n",
       "3      -0.022901  0.182600  0.223812  0.026714 -0.053800  0.222471 -0.118742  \n",
       "4       0.064305  0.232668  0.248519  0.033642 -0.032123  0.166828 -0.069573  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673  0.084714  0.211275  0.227555 -0.118160  0.228542  0.205625 -0.052172  \n",
       "119674  0.014660  0.181886  0.226069  0.073899  0.003756  0.171068 -0.157920  \n",
       "119675  0.110919  0.291915  0.228631 -0.018176 -0.035796  0.095518 -0.125593  \n",
       "119676 -0.009973  0.201993  0.227549 -0.009073 -0.000157  0.168123 -0.176607  \n",
       "119677  0.051147  0.148358  0.266412  0.074912  0.055924  0.182045 -0.136412  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train_IH, X_test_IH, y_train_IH, y_test_IH = train_test_split(train_df[\"clean_text\"],train_df[\"identity_hate\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw_IH = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v_IH = modelw_IH.transform(X_train_IH)\n",
    "X_test_vectors_w2v_IH = modelw_IH.transform(X_test_IH)\n",
    "print(X_train_vectors_w2v_IH.shape, X_test_vectors_w2v_IH.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v_IH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9517128146980427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     39554\n",
      "           1       0.62      0.14      0.23       339\n",
      "\n",
      "    accuracy                           0.99     39893\n",
      "   macro avg       0.81      0.57      0.61     39893\n",
      "weighted avg       0.99      0.99      0.99     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "IdenHate_model = RandomForestClassifier(n_jobs = -1, random_state = 0, class_weight= 'balanced', max_features= 'sqrt', min_samples_leaf= 2)\n",
    "IdenHate_model.fit(X_train_vectors_w2v_IH, y_train_IH)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "IH_y_predict = IdenHate_model.predict(X_test_vectors_w2v_IH)\n",
    "IH_y_prob = IdenHate_model.predict_proba(X_test_vectors_w2v_IH)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_IH, IH_y_prob)\n",
    "IH_roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', IH_roc_auc)  \n",
    "\n",
    "print(classification_report(y_test_IH, IH_y_predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obscene Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 300) (39893, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049615</td>\n",
       "      <td>0.215741</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.061210</td>\n",
       "      <td>-0.199817</td>\n",
       "      <td>-0.311428</td>\n",
       "      <td>0.219244</td>\n",
       "      <td>0.243215</td>\n",
       "      <td>-0.096844</td>\n",
       "      <td>-0.156019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.159593</td>\n",
       "      <td>0.078909</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>0.208511</td>\n",
       "      <td>0.197792</td>\n",
       "      <td>0.096225</td>\n",
       "      <td>-0.068411</td>\n",
       "      <td>0.149097</td>\n",
       "      <td>-0.126187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.122663</td>\n",
       "      <td>0.170981</td>\n",
       "      <td>0.052844</td>\n",
       "      <td>-0.082948</td>\n",
       "      <td>-0.082208</td>\n",
       "      <td>-0.227492</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.261226</td>\n",
       "      <td>-0.188627</td>\n",
       "      <td>-0.103327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.237499</td>\n",
       "      <td>0.066337</td>\n",
       "      <td>0.037196</td>\n",
       "      <td>0.152030</td>\n",
       "      <td>0.229269</td>\n",
       "      <td>0.100410</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.194134</td>\n",
       "      <td>-0.136206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.179589</td>\n",
       "      <td>0.206105</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>-0.007822</td>\n",
       "      <td>-0.257220</td>\n",
       "      <td>0.158980</td>\n",
       "      <td>0.270103</td>\n",
       "      <td>-0.047700</td>\n",
       "      <td>-0.241805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084374</td>\n",
       "      <td>0.167919</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>0.154459</td>\n",
       "      <td>0.196551</td>\n",
       "      <td>0.126292</td>\n",
       "      <td>-0.111216</td>\n",
       "      <td>0.120957</td>\n",
       "      <td>-0.035853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.110109</td>\n",
       "      <td>0.179623</td>\n",
       "      <td>0.063774</td>\n",
       "      <td>-0.056544</td>\n",
       "      <td>-0.092911</td>\n",
       "      <td>-0.280632</td>\n",
       "      <td>0.245265</td>\n",
       "      <td>0.314175</td>\n",
       "      <td>-0.142379</td>\n",
       "      <td>-0.150216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149075</td>\n",
       "      <td>0.211355</td>\n",
       "      <td>0.130250</td>\n",
       "      <td>-0.008173</td>\n",
       "      <td>0.173107</td>\n",
       "      <td>0.277009</td>\n",
       "      <td>0.038009</td>\n",
       "      <td>0.053995</td>\n",
       "      <td>0.214243</td>\n",
       "      <td>-0.151781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.121343</td>\n",
       "      <td>0.149354</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>-0.054425</td>\n",
       "      <td>-0.072285</td>\n",
       "      <td>-0.240403</td>\n",
       "      <td>0.286311</td>\n",
       "      <td>0.252530</td>\n",
       "      <td>-0.097521</td>\n",
       "      <td>-0.052476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.218345</td>\n",
       "      <td>0.177290</td>\n",
       "      <td>0.066034</td>\n",
       "      <td>0.196436</td>\n",
       "      <td>0.224860</td>\n",
       "      <td>0.015666</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.243052</td>\n",
       "      <td>-0.119816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>-0.106910</td>\n",
       "      <td>0.385495</td>\n",
       "      <td>-0.148397</td>\n",
       "      <td>-0.109631</td>\n",
       "      <td>-0.094599</td>\n",
       "      <td>-0.210579</td>\n",
       "      <td>0.201228</td>\n",
       "      <td>0.340012</td>\n",
       "      <td>-0.176492</td>\n",
       "      <td>-0.105761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003327</td>\n",
       "      <td>0.240437</td>\n",
       "      <td>0.173783</td>\n",
       "      <td>-0.099739</td>\n",
       "      <td>0.238892</td>\n",
       "      <td>0.247729</td>\n",
       "      <td>-0.050606</td>\n",
       "      <td>0.092592</td>\n",
       "      <td>0.057311</td>\n",
       "      <td>-0.168283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.086480</td>\n",
       "      <td>0.143127</td>\n",
       "      <td>-0.022520</td>\n",
       "      <td>-0.018202</td>\n",
       "      <td>-0.155187</td>\n",
       "      <td>-0.125961</td>\n",
       "      <td>0.052696</td>\n",
       "      <td>0.255869</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>-0.125462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030063</td>\n",
       "      <td>0.108407</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.254780</td>\n",
       "      <td>0.353115</td>\n",
       "      <td>0.028856</td>\n",
       "      <td>0.023379</td>\n",
       "      <td>0.054675</td>\n",
       "      <td>-0.009261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>-0.066630</td>\n",
       "      <td>0.216254</td>\n",
       "      <td>0.079079</td>\n",
       "      <td>-0.196406</td>\n",
       "      <td>-0.245198</td>\n",
       "      <td>-0.210012</td>\n",
       "      <td>0.111040</td>\n",
       "      <td>0.363143</td>\n",
       "      <td>-0.155034</td>\n",
       "      <td>-0.194449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.176475</td>\n",
       "      <td>0.123415</td>\n",
       "      <td>0.103965</td>\n",
       "      <td>0.310288</td>\n",
       "      <td>-0.037128</td>\n",
       "      <td>0.115647</td>\n",
       "      <td>0.142655</td>\n",
       "      <td>-0.052012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.114732</td>\n",
       "      <td>0.259318</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.060879</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.148151</td>\n",
       "      <td>0.161176</td>\n",
       "      <td>0.321447</td>\n",
       "      <td>-0.041287</td>\n",
       "      <td>-0.130298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149843</td>\n",
       "      <td>0.218250</td>\n",
       "      <td>0.104249</td>\n",
       "      <td>-0.075515</td>\n",
       "      <td>0.166044</td>\n",
       "      <td>0.228244</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>-0.085790</td>\n",
       "      <td>0.179109</td>\n",
       "      <td>-0.093916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.109260</td>\n",
       "      <td>0.341538</td>\n",
       "      <td>-0.043527</td>\n",
       "      <td>-0.065643</td>\n",
       "      <td>-0.056140</td>\n",
       "      <td>-0.133384</td>\n",
       "      <td>0.093650</td>\n",
       "      <td>0.203484</td>\n",
       "      <td>-0.106886</td>\n",
       "      <td>-0.138651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134076</td>\n",
       "      <td>0.202970</td>\n",
       "      <td>0.117657</td>\n",
       "      <td>-0.013965</td>\n",
       "      <td>0.121885</td>\n",
       "      <td>0.162161</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.220213</td>\n",
       "      <td>-0.117905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.049615  0.215741 -0.000521 -0.061210 -0.199817 -0.311428  0.219244   \n",
       "1      -0.122663  0.170981  0.052844 -0.082948 -0.082208 -0.227492  0.222201   \n",
       "2      -0.179589  0.206105  0.006614 -0.000289 -0.007822 -0.257220  0.158980   \n",
       "3      -0.110109  0.179623  0.063774 -0.056544 -0.092911 -0.280632  0.245265   \n",
       "4      -0.121343  0.149354  0.002266 -0.054425 -0.072285 -0.240403  0.286311   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673 -0.106910  0.385495 -0.148397 -0.109631 -0.094599 -0.210579  0.201228   \n",
       "119674 -0.086480  0.143127 -0.022520 -0.018202 -0.155187 -0.125961  0.052696   \n",
       "119675 -0.066630  0.216254  0.079079 -0.196406 -0.245198 -0.210012  0.111040   \n",
       "119676 -0.114732  0.259318  0.112030  0.060879 -0.063481 -0.148151  0.161176   \n",
       "119677 -0.109260  0.341538 -0.043527 -0.065643 -0.056140 -0.133384  0.093650   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.243215 -0.096844 -0.156019  ...  0.163499  0.159593  0.078909   \n",
       "1       0.261226 -0.188627 -0.103327  ...  0.116167  0.237499  0.066337   \n",
       "2       0.270103 -0.047700 -0.241805  ...  0.084374  0.167919  0.024127   \n",
       "3       0.314175 -0.142379 -0.150216  ...  0.149075  0.211355  0.130250   \n",
       "4       0.252530 -0.097521 -0.052476  ...  0.064137  0.218345  0.177290   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.340012 -0.176492 -0.105761  ... -0.003327  0.240437  0.173783   \n",
       "119674  0.255869  0.026551 -0.125462  ...  0.030063  0.108407  0.147904   \n",
       "119675  0.363143 -0.155034 -0.194449  ...  0.124836  0.053204  0.176475   \n",
       "119676  0.321447 -0.041287 -0.130298  ...  0.149843  0.218250  0.104249   \n",
       "119677  0.203484 -0.106886 -0.138651  ...  0.134076  0.202970  0.117657   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.051033  0.208511  0.197792  0.096225 -0.068411  0.149097 -0.126187  \n",
       "1       0.037196  0.152030  0.229269  0.100410 -0.015768  0.194134 -0.136206  \n",
       "2       0.018471  0.154459  0.196551  0.126292 -0.111216  0.120957 -0.035853  \n",
       "3      -0.008173  0.173107  0.277009  0.038009  0.053995  0.214243 -0.151781  \n",
       "4       0.066034  0.196436  0.224860  0.015666 -0.000937  0.243052 -0.119816  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673 -0.099739  0.238892  0.247729 -0.050606  0.092592  0.057311 -0.168283  \n",
       "119674  0.026417  0.254780  0.353115  0.028856  0.023379  0.054675 -0.009261  \n",
       "119675  0.123415  0.103965  0.310288 -0.037128  0.115647  0.142655 -0.052012  \n",
       "119676 -0.075515  0.166044  0.228244  0.003438 -0.085790  0.179109 -0.093916  \n",
       "119677 -0.013965  0.121885  0.162161  0.011833  0.000614  0.220213 -0.117905  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_OB, X_test_OB, y_train_OB, y_test_OB = train_test_split(train_df[\"clean_text\"],train_df[\"obscene\"])\n",
    "modelw_OB = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "\n",
    "X_train_vectors_w2v_OB = modelw_OB.transform(X_train_OB)\n",
    "X_test_vectors_w2v_OB = modelw_OB.transform(X_test_OB)\n",
    "\n",
    "print(X_train_vectors_w2v_OB.shape, X_test_vectors_w2v_OB.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v_OB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9457867834976268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     37818\n",
      "           1       0.90      0.47      0.62      2075\n",
      "\n",
      "    accuracy                           0.97     39893\n",
      "   macro avg       0.94      0.73      0.80     39893\n",
      "weighted avg       0.97      0.97      0.97     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "OB_model = RandomForestClassifier()\n",
    "OB_model.fit(X_train_vectors_w2v_OB, y_train_OB)\n",
    "\n",
    "y_predict_OB = OB_model.predict(X_test_vectors_w2v_OB)\n",
    "y_prob_OB = OB_model.predict_proba(X_test_vectors_w2v_OB)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_OB, y_prob_OB)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc) \n",
    "print(classification_report(y_test_OB, y_predict_OB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insult Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678, 300) (39893, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.076299</td>\n",
       "      <td>0.177462</td>\n",
       "      <td>0.048670</td>\n",
       "      <td>0.109725</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>-0.271021</td>\n",
       "      <td>0.112491</td>\n",
       "      <td>0.287859</td>\n",
       "      <td>-0.092511</td>\n",
       "      <td>-0.124849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045582</td>\n",
       "      <td>0.283879</td>\n",
       "      <td>0.123273</td>\n",
       "      <td>-0.108078</td>\n",
       "      <td>0.217287</td>\n",
       "      <td>0.185666</td>\n",
       "      <td>0.011218</td>\n",
       "      <td>-0.019935</td>\n",
       "      <td>0.222594</td>\n",
       "      <td>-0.054233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.136561</td>\n",
       "      <td>0.122760</td>\n",
       "      <td>0.071097</td>\n",
       "      <td>0.028517</td>\n",
       "      <td>-0.036605</td>\n",
       "      <td>-0.181885</td>\n",
       "      <td>0.193082</td>\n",
       "      <td>0.262406</td>\n",
       "      <td>-0.019150</td>\n",
       "      <td>-0.112978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013424</td>\n",
       "      <td>0.276550</td>\n",
       "      <td>0.272130</td>\n",
       "      <td>0.118734</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>0.146708</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>-0.063874</td>\n",
       "      <td>0.184481</td>\n",
       "      <td>-0.043920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.075830</td>\n",
       "      <td>0.247128</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>-0.042547</td>\n",
       "      <td>-0.026215</td>\n",
       "      <td>-0.168398</td>\n",
       "      <td>0.209457</td>\n",
       "      <td>0.276571</td>\n",
       "      <td>-0.100684</td>\n",
       "      <td>-0.177132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076001</td>\n",
       "      <td>0.218922</td>\n",
       "      <td>0.123111</td>\n",
       "      <td>-0.012572</td>\n",
       "      <td>0.182064</td>\n",
       "      <td>0.180949</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.085982</td>\n",
       "      <td>0.132886</td>\n",
       "      <td>-0.194817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.116194</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>0.048064</td>\n",
       "      <td>-0.054991</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>-0.268833</td>\n",
       "      <td>0.138259</td>\n",
       "      <td>0.278334</td>\n",
       "      <td>-0.116326</td>\n",
       "      <td>-0.128179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035605</td>\n",
       "      <td>0.269585</td>\n",
       "      <td>0.147867</td>\n",
       "      <td>0.059362</td>\n",
       "      <td>0.200618</td>\n",
       "      <td>0.251813</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>-0.044460</td>\n",
       "      <td>0.155468</td>\n",
       "      <td>-0.108137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025117</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>-0.030581</td>\n",
       "      <td>-0.061906</td>\n",
       "      <td>-0.049409</td>\n",
       "      <td>-0.246077</td>\n",
       "      <td>0.126151</td>\n",
       "      <td>0.226484</td>\n",
       "      <td>-0.185278</td>\n",
       "      <td>-0.121754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169352</td>\n",
       "      <td>0.197433</td>\n",
       "      <td>0.184787</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>0.226106</td>\n",
       "      <td>0.181110</td>\n",
       "      <td>0.027135</td>\n",
       "      <td>-0.090710</td>\n",
       "      <td>0.193612</td>\n",
       "      <td>-0.123254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119673</th>\n",
       "      <td>-0.034985</td>\n",
       "      <td>0.183851</td>\n",
       "      <td>0.027355</td>\n",
       "      <td>0.041044</td>\n",
       "      <td>-0.073239</td>\n",
       "      <td>-0.054226</td>\n",
       "      <td>0.205180</td>\n",
       "      <td>0.224517</td>\n",
       "      <td>-0.013038</td>\n",
       "      <td>-0.177203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119685</td>\n",
       "      <td>0.230541</td>\n",
       "      <td>0.102672</td>\n",
       "      <td>0.116155</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.221424</td>\n",
       "      <td>0.052411</td>\n",
       "      <td>-0.023720</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>-0.076871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119674</th>\n",
       "      <td>-0.136327</td>\n",
       "      <td>0.266768</td>\n",
       "      <td>-0.034069</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>-0.164965</td>\n",
       "      <td>-0.318951</td>\n",
       "      <td>0.138575</td>\n",
       "      <td>0.298791</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>-0.120991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085792</td>\n",
       "      <td>0.199125</td>\n",
       "      <td>0.046020</td>\n",
       "      <td>0.216149</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.312604</td>\n",
       "      <td>0.025662</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>-0.098254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119675</th>\n",
       "      <td>-0.072429</td>\n",
       "      <td>0.220032</td>\n",
       "      <td>0.028851</td>\n",
       "      <td>-0.011362</td>\n",
       "      <td>-0.117682</td>\n",
       "      <td>-0.176594</td>\n",
       "      <td>0.187307</td>\n",
       "      <td>0.234107</td>\n",
       "      <td>-0.099513</td>\n",
       "      <td>-0.064842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053277</td>\n",
       "      <td>0.205032</td>\n",
       "      <td>0.090530</td>\n",
       "      <td>-0.009416</td>\n",
       "      <td>0.170003</td>\n",
       "      <td>0.229131</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>-0.032801</td>\n",
       "      <td>0.167264</td>\n",
       "      <td>-0.096150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119676</th>\n",
       "      <td>-0.121562</td>\n",
       "      <td>0.145759</td>\n",
       "      <td>-0.023254</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>-0.086182</td>\n",
       "      <td>-0.108836</td>\n",
       "      <td>0.237569</td>\n",
       "      <td>0.210972</td>\n",
       "      <td>-0.102414</td>\n",
       "      <td>-0.080773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117111</td>\n",
       "      <td>0.208172</td>\n",
       "      <td>0.046143</td>\n",
       "      <td>0.035933</td>\n",
       "      <td>0.201731</td>\n",
       "      <td>0.278258</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>-0.012326</td>\n",
       "      <td>0.148719</td>\n",
       "      <td>-0.118475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119677</th>\n",
       "      <td>-0.211790</td>\n",
       "      <td>0.217323</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>-0.066941</td>\n",
       "      <td>-0.072800</td>\n",
       "      <td>-0.260827</td>\n",
       "      <td>0.324417</td>\n",
       "      <td>0.282756</td>\n",
       "      <td>-0.179517</td>\n",
       "      <td>-0.187717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158530</td>\n",
       "      <td>0.289278</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>-0.054349</td>\n",
       "      <td>0.252568</td>\n",
       "      <td>0.317395</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.136205</td>\n",
       "      <td>0.192639</td>\n",
       "      <td>-0.060405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119678 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.076299  0.177462  0.048670  0.109725  0.019200 -0.271021  0.112491   \n",
       "1      -0.136561  0.122760  0.071097  0.028517 -0.036605 -0.181885  0.193082   \n",
       "2      -0.075830  0.247128  0.005949 -0.042547 -0.026215 -0.168398  0.209457   \n",
       "3      -0.116194  0.167378  0.048064 -0.054991 -0.000572 -0.268833  0.138259   \n",
       "4      -0.025117  0.134579 -0.030581 -0.061906 -0.049409 -0.246077  0.126151   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "119673 -0.034985  0.183851  0.027355  0.041044 -0.073239 -0.054226  0.205180   \n",
       "119674 -0.136327  0.266768 -0.034069  0.023148 -0.164965 -0.318951  0.138575   \n",
       "119675 -0.072429  0.220032  0.028851 -0.011362 -0.117682 -0.176594  0.187307   \n",
       "119676 -0.121562  0.145759 -0.023254  0.020397 -0.086182 -0.108836  0.237569   \n",
       "119677 -0.211790  0.217323  0.087500 -0.066941 -0.072800 -0.260827  0.324417   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.287859 -0.092511 -0.124849  ...  0.045582  0.283879  0.123273   \n",
       "1       0.262406 -0.019150 -0.112978  ...  0.013424  0.276550  0.272130   \n",
       "2       0.276571 -0.100684 -0.177132  ...  0.076001  0.218922  0.123111   \n",
       "3       0.278334 -0.116326 -0.128179  ...  0.035605  0.269585  0.147867   \n",
       "4       0.226484 -0.185278 -0.121754  ...  0.169352  0.197433  0.184787   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "119673  0.224517 -0.013038 -0.177203  ...  0.119685  0.230541  0.102672   \n",
       "119674  0.298791 -0.017950 -0.120991  ...  0.085792  0.199125  0.046020   \n",
       "119675  0.234107 -0.099513 -0.064842  ...  0.053277  0.205032  0.090530   \n",
       "119676  0.210972 -0.102414 -0.080773  ...  0.117111  0.208172  0.046143   \n",
       "119677  0.282756 -0.179517 -0.187717  ...  0.158530  0.289278  0.037725   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0      -0.108078  0.217287  0.185666  0.011218 -0.019935  0.222594 -0.054233  \n",
       "1       0.118734  0.190579  0.146708  0.008921 -0.063874  0.184481 -0.043920  \n",
       "2      -0.012572  0.182064  0.180949 -0.005001  0.085982  0.132886 -0.194817  \n",
       "3       0.059362  0.200618  0.251813  0.020061 -0.044460  0.155468 -0.108137  \n",
       "4      -0.036146  0.226106  0.181110  0.027135 -0.090710  0.193612 -0.123254  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "119673  0.116155  0.224787  0.221424  0.052411 -0.023720  0.147871 -0.076871  \n",
       "119674  0.216149  0.263708  0.312604  0.025662 -0.019392  0.034530 -0.098254  \n",
       "119675 -0.009416  0.170003  0.229131  0.042517 -0.032801  0.167264 -0.096150  \n",
       "119676  0.035933  0.201731  0.278258  0.009171 -0.012326  0.148719 -0.118475  \n",
       "119677 -0.054349  0.252568  0.317395  0.002896  0.136205  0.192639 -0.060405  \n",
       "\n",
       "[119678 rows x 300 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_In, X_test_In, y_train_In, y_test_In = train_test_split(train_df[\"clean_text\"],train_df[\"insult\"])\n",
    "modelw_In = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "\n",
    "X_train_vectors_w2v_In = modelw_In.transform(X_train_In)\n",
    "X_test_vectors_w2v_In = modelw_In.transform(X_test_In)\n",
    "\n",
    "print(X_train_vectors_w2v_In.shape, X_test_vectors_w2v_In.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v_In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9379047116372159\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     37947\n",
      "           1       0.81      0.42      0.55      1946\n",
      "\n",
      "    accuracy                           0.97     39893\n",
      "   macro avg       0.89      0.71      0.77     39893\n",
      "weighted avg       0.96      0.97      0.96     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "Insult_model = RandomForestClassifier()\n",
    "Insult_model.fit(X_train_vectors_w2v_In, y_train_In)\n",
    "\n",
    "y_predict_In = Insult_model.predict(X_test_vectors_w2v_In)\n",
    "y_prob_In = Insult_model.predict_proba(X_test_vectors_w2v_In)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_In, y_prob_In)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc) \n",
    "print(classification_report(y_test_In, y_predict_In))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although in general the accuracy of our final model is high, it is somehow having difficulties predicting target variables that are very low in counts such as the severe_toxic, threat and identity_hate as reflected on their f1-scores of 0.32, 0.14 and 0.25, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "tok = lemmaTokenizer(stop_words=stop_words)\n",
    "test_df['clean_text'] = test_df['comment_text'].apply(lambda x: (tok(x)))\n",
    "# create Word2vec model\n",
    "test = Word2Vec(test_df['clean_text'], min_count = 2, vector_size = 300, sg = 1)\n",
    "test_w2v = dict(zip(test.wv.index_to_key, test.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055201</td>\n",
       "      <td>-0.150612</td>\n",
       "      <td>-0.035569</td>\n",
       "      <td>-0.152634</td>\n",
       "      <td>0.152053</td>\n",
       "      <td>0.029742</td>\n",
       "      <td>-0.063537</td>\n",
       "      <td>0.299461</td>\n",
       "      <td>0.184763</td>\n",
       "      <td>0.108145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.132340</td>\n",
       "      <td>0.021967</td>\n",
       "      <td>0.095133</td>\n",
       "      <td>0.202271</td>\n",
       "      <td>0.058625</td>\n",
       "      <td>0.215362</td>\n",
       "      <td>0.119273</td>\n",
       "      <td>0.093392</td>\n",
       "      <td>0.018274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.114263</td>\n",
       "      <td>-0.079369</td>\n",
       "      <td>-0.211238</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>0.038758</td>\n",
       "      <td>-0.047164</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.265045</td>\n",
       "      <td>0.127202</td>\n",
       "      <td>0.109241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053674</td>\n",
       "      <td>0.172464</td>\n",
       "      <td>0.042809</td>\n",
       "      <td>0.058235</td>\n",
       "      <td>0.229962</td>\n",
       "      <td>0.154217</td>\n",
       "      <td>0.228001</td>\n",
       "      <td>-0.065258</td>\n",
       "      <td>0.096375</td>\n",
       "      <td>-0.241245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.025808</td>\n",
       "      <td>-0.040806</td>\n",
       "      <td>-0.265002</td>\n",
       "      <td>0.091568</td>\n",
       "      <td>0.196788</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.172367</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.164147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022060</td>\n",
       "      <td>0.322563</td>\n",
       "      <td>0.071119</td>\n",
       "      <td>-0.038720</td>\n",
       "      <td>0.225001</td>\n",
       "      <td>0.085738</td>\n",
       "      <td>0.150080</td>\n",
       "      <td>-0.221515</td>\n",
       "      <td>0.095214</td>\n",
       "      <td>-0.205111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.048865</td>\n",
       "      <td>-0.113270</td>\n",
       "      <td>-0.221400</td>\n",
       "      <td>0.107247</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>-0.047481</td>\n",
       "      <td>0.152740</td>\n",
       "      <td>0.359466</td>\n",
       "      <td>0.091587</td>\n",
       "      <td>0.191358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084328</td>\n",
       "      <td>0.298638</td>\n",
       "      <td>-0.054102</td>\n",
       "      <td>0.085479</td>\n",
       "      <td>0.279424</td>\n",
       "      <td>0.110128</td>\n",
       "      <td>0.201924</td>\n",
       "      <td>-0.057929</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>-0.216837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009670</td>\n",
       "      <td>-0.103263</td>\n",
       "      <td>-0.288295</td>\n",
       "      <td>-0.088466</td>\n",
       "      <td>-0.028198</td>\n",
       "      <td>0.040992</td>\n",
       "      <td>0.156282</td>\n",
       "      <td>0.353921</td>\n",
       "      <td>0.110072</td>\n",
       "      <td>-0.002937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126334</td>\n",
       "      <td>0.330577</td>\n",
       "      <td>0.022643</td>\n",
       "      <td>0.094289</td>\n",
       "      <td>0.288221</td>\n",
       "      <td>0.164517</td>\n",
       "      <td>0.112930</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>0.120101</td>\n",
       "      <td>-0.342887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>-0.027394</td>\n",
       "      <td>-0.132996</td>\n",
       "      <td>-0.319799</td>\n",
       "      <td>-0.161095</td>\n",
       "      <td>-0.057607</td>\n",
       "      <td>-0.101171</td>\n",
       "      <td>0.118599</td>\n",
       "      <td>0.360904</td>\n",
       "      <td>0.193937</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.299337</td>\n",
       "      <td>0.027422</td>\n",
       "      <td>0.073347</td>\n",
       "      <td>0.231744</td>\n",
       "      <td>0.186034</td>\n",
       "      <td>0.131905</td>\n",
       "      <td>-0.011249</td>\n",
       "      <td>-0.026247</td>\n",
       "      <td>-0.054599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.006466</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>-0.005103</td>\n",
       "      <td>0.045230</td>\n",
       "      <td>-0.004657</td>\n",
       "      <td>0.075040</td>\n",
       "      <td>0.265033</td>\n",
       "      <td>0.098261</td>\n",
       "      <td>0.096755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>0.187065</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.083947</td>\n",
       "      <td>0.277969</td>\n",
       "      <td>0.121471</td>\n",
       "      <td>0.182416</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.182431</td>\n",
       "      <td>-0.111798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>-0.068146</td>\n",
       "      <td>-0.089727</td>\n",
       "      <td>-0.189245</td>\n",
       "      <td>0.027235</td>\n",
       "      <td>0.038152</td>\n",
       "      <td>-0.073132</td>\n",
       "      <td>0.089066</td>\n",
       "      <td>0.237970</td>\n",
       "      <td>0.108128</td>\n",
       "      <td>0.164419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.259653</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>0.071231</td>\n",
       "      <td>0.211157</td>\n",
       "      <td>0.225671</td>\n",
       "      <td>0.180421</td>\n",
       "      <td>-0.046809</td>\n",
       "      <td>0.136371</td>\n",
       "      <td>-0.129765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>-0.047476</td>\n",
       "      <td>-0.013166</td>\n",
       "      <td>-0.173263</td>\n",
       "      <td>0.044133</td>\n",
       "      <td>0.055374</td>\n",
       "      <td>-0.059368</td>\n",
       "      <td>0.026533</td>\n",
       "      <td>0.219156</td>\n",
       "      <td>0.121567</td>\n",
       "      <td>0.114140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.271990</td>\n",
       "      <td>0.062070</td>\n",
       "      <td>0.040823</td>\n",
       "      <td>0.230648</td>\n",
       "      <td>0.268138</td>\n",
       "      <td>0.175075</td>\n",
       "      <td>-0.006215</td>\n",
       "      <td>0.176035</td>\n",
       "      <td>-0.125672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>-0.079174</td>\n",
       "      <td>-0.085840</td>\n",
       "      <td>-0.217438</td>\n",
       "      <td>-0.020168</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>-0.044388</td>\n",
       "      <td>0.087764</td>\n",
       "      <td>0.268158</td>\n",
       "      <td>0.185041</td>\n",
       "      <td>0.072739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044757</td>\n",
       "      <td>0.260581</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.072096</td>\n",
       "      <td>0.248756</td>\n",
       "      <td>0.104729</td>\n",
       "      <td>0.075895</td>\n",
       "      <td>-0.039337</td>\n",
       "      <td>0.143242</td>\n",
       "      <td>-0.219280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       "0      -0.055201 -0.150612 -0.035569 -0.152634  0.152053  0.029742 -0.063537   \n",
       "1      -0.114263 -0.079369 -0.211238  0.093145  0.038758 -0.047164  0.027371   \n",
       "2      -0.025808 -0.040806 -0.265002  0.091568  0.196788  0.022114  0.037464   \n",
       "3      -0.048865 -0.113270 -0.221400  0.107247  0.036995 -0.047481  0.152740   \n",
       "4       0.009670 -0.103263 -0.288295 -0.088466 -0.028198  0.040992  0.156282   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "153159 -0.027394 -0.132996 -0.319799 -0.161095 -0.057607 -0.101171  0.118599   \n",
       "153160 -0.003697 -0.006466  0.012573 -0.005103  0.045230 -0.004657  0.075040   \n",
       "153161 -0.068146 -0.089727 -0.189245  0.027235  0.038152 -0.073132  0.089066   \n",
       "153162 -0.047476 -0.013166 -0.173263  0.044133  0.055374 -0.059368  0.026533   \n",
       "153163 -0.079174 -0.085840 -0.217438 -0.020168  0.071732 -0.044388  0.087764   \n",
       "\n",
       "             7         8         9    ...       290       291       292  \\\n",
       "0       0.299461  0.184763  0.108145  ...  0.017165  0.132340  0.021967   \n",
       "1       0.265045  0.127202  0.109241  ...  0.053674  0.172464  0.042809   \n",
       "2       0.172367  0.003711  0.164147  ...  0.022060  0.322563  0.071119   \n",
       "3       0.359466  0.091587  0.191358  ...  0.084328  0.298638 -0.054102   \n",
       "4       0.353921  0.110072 -0.002937  ...  0.126334  0.330577  0.022643   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "153159  0.360904  0.193937  0.024362  ...  0.023471  0.299337  0.027422   \n",
       "153160  0.265033  0.098261  0.096755  ...  0.013929  0.187065  0.000504   \n",
       "153161  0.237970  0.108128  0.164419  ...  0.032483  0.259653 -0.001374   \n",
       "153162  0.219156  0.121567  0.114140  ...  0.011572  0.271990  0.062070   \n",
       "153163  0.268158  0.185041  0.072739  ...  0.044757  0.260581  0.002376   \n",
       "\n",
       "             293       294       295       296       297       298       299  \n",
       "0       0.095133  0.202271  0.058625  0.215362  0.119273  0.093392  0.018274  \n",
       "1       0.058235  0.229962  0.154217  0.228001 -0.065258  0.096375 -0.241245  \n",
       "2      -0.038720  0.225001  0.085738  0.150080 -0.221515  0.095214 -0.205111  \n",
       "3       0.085479  0.279424  0.110128  0.201924 -0.057929  0.063105 -0.216837  \n",
       "4       0.094289  0.288221  0.164517  0.112930 -0.012753  0.120101 -0.342887  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "153159  0.073347  0.231744  0.186034  0.131905 -0.011249 -0.026247 -0.054599  \n",
       "153160  0.083947  0.277969  0.121471  0.182416  0.017408  0.182431 -0.111798  \n",
       "153161  0.071231  0.211157  0.225671  0.180421 -0.046809  0.136371 -0.129765  \n",
       "153162  0.040823  0.230648  0.268138  0.175075 -0.006215  0.176035 -0.125672  \n",
       "153163  0.072096  0.248756  0.104729  0.075895 -0.039337  0.143242 -0.219280  \n",
       "\n",
       "[153164 rows x 300 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testw = MeanEmbeddingVectorizer(test_w2v)\n",
    "test_vectors = testw.transform(test_df[\"clean_text\"])\n",
    "print(test_vectors.shape)\n",
    "pd.DataFrame(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    152856\n",
       "1       308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_predict = toxic_model.predict(test_vectors)\n",
    "toxic = pd.DataFrame(toxic_predict)\n",
    "toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    152856\n",
       "1       308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severetoxic_predict = severetoxic_model.predict(test_vectors)\n",
    "severe_toxic = pd.DataFrame(severetoxic_predict)\n",
    "severe_toxic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    153164\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threat_predict = threat_model.predict(test_vectors)\n",
    "threat = pd.DataFrame(threat_predict)\n",
    "threat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    152856\n",
       "1       308\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idenhate_predict = IdenHate_model.predict(test_vectors)\n",
    "identity_hate = pd.DataFrame(idenhate_predict)\n",
    "identity_hate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    153117\n",
       "1        47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obscene_predict = OB_model.predict(test_vectors)\n",
    "obscene = pd.DataFrame(obscene_predict)\n",
    "obscene.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    153144\n",
       "1        20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insult_predict = Insult_model.predict(test_vectors)\n",
    "insult = pd.DataFrame(insult_predict)\n",
    "insult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153164 entries, 0 to 153163\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             153164 non-null  object\n",
      " 1   toxic          153164 non-null  int64 \n",
      " 2   severe_toxic   153164 non-null  int64 \n",
      " 3   obscene        153164 non-null  int64 \n",
      " 4   threat         153164 non-null  int64 \n",
      " 5   insult         153164 non-null  int64 \n",
      " 6   identity_hate  153164 non-null  int64 \n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "targets = {\n",
    "    \"toxic\": toxic,\n",
    "    \"severe_toxic\": severe_toxic,\n",
    "    \"obscene\": obscene,\n",
    "    \"threat\": threat,\n",
    "    \"insult\": insult,\n",
    "    \"identity_hate\": identity_hate}\n",
    "\n",
    "columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "df_out = pd.DataFrame(columns = ['id'] + columns)\n",
    "\n",
    "\n",
    "for i in columns :\n",
    "    df_out['id'] = test_df['id'].astype('object')\n",
    "    df_out = df_out[['id'] + columns]\n",
    "    df_out[i] = targets[i]\n",
    "\n",
    "\n",
    "df_out.to_csv('out_1.csv', index = False)\n",
    "df_out.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Details, Submission Info, and Example Submission\n",
    "\n",
    "For this project, please output your predictions in a CSV file. The structure of the CSV file should match the structure of the example below. \n",
    "\n",
    "The output should contain one row for each row of test data, complete with the columns for ID and each classification.\n",
    "\n",
    "Into Moodle please submit:\n",
    "<ul>\n",
    "<li> Your notebook file(s). I'm not going to run them, just look. \n",
    "<li> Your sample submission CSV. This will be evaluated for accuracy against the real labels; only a subset of the predictions will be scored. \n",
    "</ul>\n",
    "\n",
    "It is REALLY, REALLY, REALLY important the the structure of your output matches the specifications. The accuracies will be calculated by a script, and it is expecting a specific format. \n",
    "\n",
    "### Sample Evaluator\n",
    "\n",
    "The file prediction_evaluator.ipynb contains an example scoring function, scoreChecker. This function takes a sumbission and an answer key, loops through, and evaluates the accuracy. You can use this to verify the format of your submission. I'm going to use the same function to evaluate the accuracy of your submission, against the answer key (unless I made some mistake in this counting function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Construct dummy data for a sample output. \n",
    "#You won't do this part first, you have real data - I'm faking it. \n",
    "#Your data should have the same structure, so the CSV output is the same\n",
    "dummy_ids = [\"dfasdf234\", \"asdfgw43r52\", \"asdgtawe4\", \"wqtr215432\"]\n",
    "dummy_toxic = [0,0,0,0]\n",
    "dummy_severe = [0,0,0,0]\n",
    "dummy_obscene = [0,1,1,0]\n",
    "dummy_threat = [0,1,0,1]\n",
    "dummy_insult = [0,0,1,0]\n",
    "dummy_ident = [0,1,1,0]\n",
    "columns = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_out = pd.DataFrame( list(zip(dummy_ids, dummy_toxic, dummy_severe, dummy_obscene, dummy_threat, dummy_insult, dummy_ident)),\n",
    "                    columns=columns)\n",
    "sample_out.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write DF to CSV. Please keep the \"out.csv\" filename. Moodle will auto-preface it with an identifier when I download it. \n",
    "#This command should work with your dataframe of predictions. \n",
    "#sample_out.to_csv('out.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "The grading for this is split between accuracy and well written code:\n",
    "<ul>\n",
    "<li> 75% - Accuracy. The most accurate will get 100% on this, the others will be scaled down from there. \n",
    "<li> 25% - Code quality. Can the code be followed and made sense of - i.e. comments, sections, titles. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
